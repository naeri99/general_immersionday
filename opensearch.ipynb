{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a6b527-1cc2-4a93-ab59-cc9397a647d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2347ea6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import BasePromptTemplate\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "from langchain_core.documents import Document\n",
    "from pypdf import PdfReader\n",
    "import concurrent.futures\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from pydantic import BaseModel, Field\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47fa1678-f7f8-4524-b1ef-9a604cae1061",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab272cd2-8f12-4ccd-aec2-9265f27468e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#text를 vector로 변환\n",
    "def get_embedding(text):\n",
    "    embeddings = embedding_model.encode(text)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "34406be2-aa1a-4e2e-b9a5-003354306ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_embedding(\"안녕하세요?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c69f1e-8e6d-4bff-9df0-f5cbc481c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opensearch에 index(sql table과 유사) 생성\n",
    "def define_index(opensearch_client, index_name):\n",
    "    index_settings = {\n",
    "        \"settings\": {\n",
    "            \"index\": {  \n",
    "                \"knn\": True,                          \n",
    "                \"knn.algo_param.ef_search\": 100,      \n",
    "                \"number_of_shards\": 3,                \n",
    "                \"number_of_replicas\": 2, \n",
    "                \"analysis\": {\n",
    "                    \"analyzer\": {\n",
    "                        \"nori_analyzer\": {\n",
    "                            \"tokenizer\": \"nori_tokenizer\",\n",
    "                            \"filter\": [\"nori_stop\", \"lowercase\"]\n",
    "                        }\n",
    "                    },\n",
    "                    \"filter\": {\n",
    "                        \"nori_stop\": {\n",
    "                            \"type\": \"nori_part_of_speech\",\n",
    "                            \"stoptags\": [\"J\", \"JKS\", \"JKB\", \"JKO\", \"JKG\", \"JKC\", \"JKV\", \"JKQ\", \"JX\", \"JC\"]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },   \n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"document\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"analyzer\": \"nori_analyzer\"\n",
    "                },\n",
    "                \"doc_vector\": {\n",
    "                    \"type\": \"knn_vector\",\n",
    "                    \"dimension\": 384,\n",
    "                    \"method\": {\n",
    "                        \"name\": \"hnsw\",   \n",
    "                        \"space_type\": \"l2\",\n",
    "                        \"engine\": \"faiss\",\n",
    "                        \"parameters\": {\n",
    "                            \"ef_construction\": 128,\n",
    "                            \"m\": 16\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"source\": {\n",
    "                    \"type\": \"keyword\"  # Use keyword for exact matching\n",
    "                },\n",
    "                \"page\": {\n",
    "                    \"type\": \"integer\"\n",
    "                },\n",
    "                \"total_pages\": {\n",
    "                    \"type\": \"integer\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        response = opensearch_client.indices.create(\n",
    "            index=index_name,\n",
    "            body=index_settings\n",
    "        )\n",
    "        print(\"Index created successfully:\", response)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error creating index:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b20520f7-34e9-4aea-88fb-59b55c45ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf파일 opensearch에 적재\n",
    "def save_chunk(opensearh_client,  index_name,chunks):\n",
    "    for d in enumerate(chunks):\n",
    "        document = d[1].page_content\n",
    "        raw_embedding = get_embedding(d[1].page_content)\n",
    "        doc_vector = [float(val) for val in raw_embedding]\n",
    "        source = d[1].metadata[\"source\"]\n",
    "        page_info= d[1].metadata[\"page\"]\n",
    "        total_page=d[1].metadata[\"total_pages\"]\n",
    "    \n",
    "        # Prepare document for indexing\n",
    "        document = {\n",
    "            \"document\": document,\n",
    "            \"doc_vector\":doc_vector,\n",
    "            \"source\" : source, \n",
    "            \"page\" : page_info , \n",
    "            \"total_pages\" : total_page \n",
    "        }\n",
    "        try:\n",
    "            response = opensearh_client.index(\n",
    "                index=index_name,\n",
    "                body=document,\n",
    "                id=f\"sagemaker_doc_{d[0]}\"\n",
    "            )\n",
    "            if d[0] % 500 ==0 :\n",
    "                print(f\"Document {d[0]} indexed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error indexing document {d[0]}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e74069b2-4be4-4fb4-b5a8-b3cf58efb5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#속도를 위해 parallel하게 진행. 페이지를 부분적으로 나누어서 text파일 읽기\n",
    "def process_page_range(pdf_path, start_page, end_page):\n",
    "    \"\"\"Process a range of pages from a PDF.\"\"\"\n",
    "    # Open the PDF\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_pages = doc.page_count\n",
    "    \n",
    "    batch_docs = []\n",
    "    for i in range(start_page, end_page):\n",
    "        if i >= total_pages:\n",
    "            break\n",
    "            \n",
    "        # Get page and extract text\n",
    "        page = doc[i]\n",
    "        text = page.get_text()\n",
    "        \n",
    "        # Create Document object\n",
    "        doc_obj = Document(\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                \"source\": pdf_path,\n",
    "                \"page\": i + 1,\n",
    "                \"total_pages\": total_pages\n",
    "            }\n",
    "        )\n",
    "        batch_docs.append(doc_obj)\n",
    "    \n",
    "    return batch_docs\n",
    "    \n",
    "#pdf파일 읽기 \n",
    "def load_pdf_parallel(pdf_path, batch_size=100, max_workers=4):\n",
    "    \"\"\"Load a PDF using parallel processing.\"\"\"\n",
    "    # Get total pages\n",
    "    doc = fitz.open(pdf_path)\n",
    "    total_pages = doc.page_count\n",
    "    doc.close()\n",
    "    print(f\"PDF has {total_pages} pages in total\")\n",
    "    \n",
    "    # Create batches\n",
    "    batches = []\n",
    "    for start_page in range(0, total_pages, batch_size):\n",
    "        end_page = min(start_page + batch_size, total_pages)\n",
    "        batches.append((start_page, end_page))\n",
    "    \n",
    "    all_docs = []\n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit tasks\n",
    "        future_to_batch = {\n",
    "            executor.submit(process_page_range, pdf_path, start, end): (start, end)\n",
    "            for start, end in batches\n",
    "        }\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_batch):\n",
    "            start, end = future_to_batch[future]\n",
    "            try:\n",
    "                batch_docs = future.result()\n",
    "                all_docs.extend(batch_docs)\n",
    "                print(f\"Completed batch: pages {start+1} to {end}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing pages {start+1} to {end}: {e}\")\n",
    "    \n",
    "    # Sort by page number\n",
    "    all_docs.sort(key=lambda x: x.metadata[\"page\"])\n",
    "    \n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "110a12f5-b02e-4170-94a1-b90e793889f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to OpenSearch\n",
      "{'name': '231d6b0ab6706e8eefe31db7bf507b69', 'cluster_name': '008971635601:bank-opensearch-domain', 'cluster_uuid': 'c3SpWUfdRC2_5LzPRUmGAA', 'version': {'number': '7.10.2', 'build_type': 'tar', 'build_hash': 'unknown', 'build_date': '2025-10-02T02:20:56.818914285Z', 'build_snapshot': False, 'lucene_version': '10.2.1', 'minimum_wire_compatibility_version': '2.19.0', 'minimum_index_compatibility_version': '2.0.0'}, 'tagline': 'The OpenSearch Project: https://opensearch.org/'}\n"
     ]
    }
   ],
   "source": [
    "#opensearh client 연결 생성 및 확인\n",
    "opensearch_client = OpenSearch(\n",
    "    hosts = [{'host': 'search-bank-opensearch-domain-hz673yoao6ld43n3atan6uqvgu.ap-northeast-2.es.amazonaws.com', 'port': 443}],\n",
    "    http_auth = ('Bankadmin', 'Bankadmin123!'),  # Replace with your master credentials\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "try:\n",
    "    response = opensearch_client.info()\n",
    "    print(\"Successfully connected to OpenSearch\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72cb09a9-cd18-44f0-82cc-9b1b18bf1601",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"nori_test_index\"\n",
    "\n",
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"nori_analyzer\": {\n",
    "                    \"type\": \"nori\",\n",
    "                    \"decompound_mode\": \"mixed\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"nori_analyzer\"\n",
    "            },\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"nori_analyzer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9108c96c-eb04-412d-a55a-bd768bd29d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 's3_explain' deleted successfully: {'acknowledged': True}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    delete_response = opensearch_client.indices.delete(index=index_name)\n",
    "    print(f\"Index 's3_explain' deleted successfully: {delete_response}\")\n",
    "except Exception as delete_error:\n",
    "    print(f\"Failed to delete index 's3_explain': {delete_error}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3b8933ca-949e-42d3-8756-507baa821ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'nori_test_index'}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스 생성\n",
    "\n",
    "opensearch_client.indices.create(index=index_name, body=index_body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8cd82b28-8b12-44eb-8e94-38b10d7c0b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 테스트 문서 인덱싱\n",
    "test_docs = [\n",
    "    {\"title\": \"한국어 형태소 분석\", \"content\": \"노리 분석기는 한국어를 잘 처리합니다\"},\n",
    "    {\"title\": \"은행 업무 시스템\", \"content\": \"금융권에서 사용하는 검색 시스템입니다\"},\n",
    "    {\"title\": \"OpenSearch 활용\", \"content\": \"엘라스틱서치 호환 검색엔진입니다\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "55c0ce95-d35a-4d3c-8184-3c21d47de5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(test_docs):\n",
    "    opensearch_client.index(index=index_name, id=i+1, body=doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "65aa5238-d71a-46b5-9685-b674c1793014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "문서 1: 한국어 형태소 분석\n",
      "원문: 노리 분석기는 한국어를 잘 처리합니다\n",
      "분석된 토큰: 노리 | 분석기 | 분석 | 기 | 한국어 | 한국 | 어 | 처리\n",
      "\n",
      "문서 2: 은행 업무 시스템\n",
      "원문: 금융권에서 사용하는 검색 시스템입니다\n",
      "분석된 토큰: 금융 | 사용 | 검색 | 시스템 | 입니다 | 이\n",
      "\n",
      "문서 3: OpenSearch 활용\n",
      "원문: 엘라스틱서치 호환 검색엔진입니다\n",
      "분석된 토큰: 엘라스틱 | 서치 | 호환 | 검색 | 엔진 | 입니다 | 이\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(test_docs):\n",
    "    print(f\"\\n문서 {i+1}: {doc['title']}\")\n",
    "    print(f\"원문: {doc['content']}\")\n",
    "    \n",
    "    # 노리 analyzer로 분석\n",
    "    analyze_result = opensearch_client.indices.analyze(\n",
    "        index=index_name,\n",
    "        body={\n",
    "            \"analyzer\": \"nori_analyzer\",\n",
    "            \"text\": doc['content']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    tokens = [token['token'] for token in analyze_result['tokens']]\n",
    "    print(f\"분석된 토큰: {' | '.join(tokens)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c837db0f-728d-4d83-9a0c-970c7bbd2298",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# 4. 검색 테스트\n",
    "search_query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"content\": \"한국어\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "search_result = opensearch_client.search(index=index_name, body=search_query)\n",
    "print(f\"\\n검색 결과: {search_result['hits']['total']['value']}건\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8935ce5c-d4c7-4ef4-9463-e23aaac8cf43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe6fcd42-95f0-48ef-ae4b-fc53d297db0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF has 2716 pages in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch: pages 101 to 200\n",
      "Completed batch: pages 1 to 100\n",
      "Completed batch: pages 201 to 300\n",
      "Completed batch: pages 301 to 400\n",
      "Completed batch: pages 401 to 500\n",
      "Completed batch: pages 501 to 600\n",
      "Completed batch: pages 601 to 700\n",
      "Completed batch: pages 701 to 800\n",
      "Completed batch: pages 801 to 900\n",
      "Completed batch: pages 901 to 1000\n",
      "Completed batch: pages 1001 to 1100\n",
      "Completed batch: pages 1101 to 1200\n",
      "Completed batch: pages 1201 to 1300\n",
      "Completed batch: pages 1301 to 1400\n",
      "Completed batch: pages 1401 to 1500\n",
      "Completed batch: pages 1501 to 1600\n",
      "Completed batch: pages 1601 to 1700\n",
      "Completed batch: pages 1701 to 1800\n",
      "Completed batch: pages 1801 to 1900\n",
      "Completed batch: pages 1901 to 2000\n",
      "Completed batch: pages 2001 to 2100\n",
      "Completed batch: pages 2101 to 2200\n",
      "Completed batch: pages 2201 to 2300\n",
      "Completed batch: pages 2301 to 2400\n",
      "Completed batch: pages 2401 to 2500\n",
      "Completed batch: pages 2501 to 2600\n",
      "Completed batch: pages 2601 to 2700\n",
      "Completed batch: pages 2701 to 2716\n"
     ]
    }
   ],
   "source": [
    "#pdf파일을 chunk로 나눈다\n",
    "path=\"/home/jovyan/langtest/langraph_work_shop/data/s3-userguide.pdf\"\n",
    "page=load_pdf_parallel(path,max_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e61325a0-2acb-4546-99fb-51d8d841767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#반환된 결과는 List[Document] 형태입니다.\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "split_docs = text_splitter.split_documents(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6cbf2f30-d2eb-464d-b566-b615a15d8cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 갯수 ->  7468\n"
     ]
    }
   ],
   "source": [
    "print(\"총 갯수 -> \",len(split_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7b75f974-2a47-4ad5-850c-e3c5209ccd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 's3_explain' deleted successfully: {'acknowledged': True}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    delete_response = opensearch_client.indices.delete(index='s3_explain')\n",
    "    print(f\"Index 's3_explain' deleted successfully: {delete_response}\")\n",
    "except Exception as delete_error:\n",
    "    print(f\"Failed to delete index 's3_explain': {delete_error}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8b9e93b2-0c34-4530-8788-13581dbc4991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created successfully: {'acknowledged': True, 'shards_acknowledged': True, 'index': 's3_explain'}\n"
     ]
    }
   ],
   "source": [
    "#index 생성\n",
    "define_index(opensearch_client, \"s3_explain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5580d4db-458a-44e1-bd85-d28cf617395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 indexed successfully\n",
      "Document 500 indexed successfully\n",
      "Document 1000 indexed successfully\n"
     ]
    }
   ],
   "source": [
    "save_chunk(opensearch_client, \"s3_explain\" ,  split_docs[1000:2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "af5be344-c02e-49d9-aa2b-3391c2afd5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BM25 search ( nori_analyzer를 이용하여 형태소 분석을 진행한후 텍스트 유사도 측정 )\n",
    "def simple_text_search(client, search_text, index_name=\"s3_explain\", k=10):\n",
    "    try:\n",
    "        # Simple text search query\n",
    "        text_query = {\n",
    "            \"size\": k,  # Number of results to return\n",
    "            \"_source\": {\n",
    "                \"excludes\": [\"doc_vector\"]  # Exclude vector field from results\n",
    "            },\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"document\": {\n",
    "                        \"query\": search_text,\n",
    "                        \"analyzer\": \"nori_analyzer\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Execute search\n",
    "        response = client.search(\n",
    "            index=index_name,\n",
    "            body=text_query\n",
    "        )\n",
    "\n",
    "\n",
    "        documents = []\n",
    "        if response.get(\"hits\", {}).get(\"hits\", []):\n",
    "            search_results = normalize_search_results(response) \n",
    "            for res in search_results[\"hits\"][\"hits\"]:\n",
    "                source = res['_source']\n",
    "                page_content = {k: source[k] for k in source if k != \"table_summary_v\"}\n",
    "                metadata = {\"id\": res['_id']}\n",
    "                score = res['_score']  # Get the score from the search result\n",
    "                documents.append((Document(page_content=json.dumps(page_content, ensure_ascii=False), metadata=metadata), score))\n",
    "        return documents  \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "#BM25 search의 score는 0.1 to 15.0 임으로 normazlie(0~1)를 해야함 (추후 vector search와 score 비교를 하기위해)\n",
    "def normalize_search_results(search_results):\n",
    "        hits = (search_results[\"hits\"][\"hits\"])\n",
    "        max_score = float(search_results[\"hits\"][\"max_score\"])\n",
    "        for hit in hits:\n",
    "            hit[\"_score\"] = float(hit[\"_score\"]) / max_score\n",
    "        search_results[\"hits\"][\"max_score\"] = hits[0][\"_score\"]\n",
    "        search_results[\"hits\"][\"hits\"] = hits\n",
    "        return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "429d1369-34e0-4602-84f8-a3d69c4945b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector 유사도 검색 \n",
    "def vector_search(client, embedding_vector, index_name=\"s3_explain\", k=3):\n",
    "    try:\n",
    "        # KNN vector search query\n",
    "        vector_query = {\n",
    "            \"size\": k,\n",
    "            \"_source\": {\n",
    "                \"excludes\": [\"doc_vector\"]  # Exclude vector field from results\n",
    "            },\n",
    "            \"query\": {\n",
    "                \"knn\": {\n",
    "                    \"doc_vector\": {\n",
    "                        \"vector\": embedding_vector,\n",
    "                        \"k\": k\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Execute the search\n",
    "        response = client.search(\n",
    "            index=index_name,\n",
    "            body=vector_query\n",
    "        )\n",
    "\n",
    "        documents = []\n",
    "        for res in response[\"hits\"][\"hits\"]:\n",
    "            source = res['_source']\n",
    "            page_content = {k: source[k] for k in source if k != \"vector\"}\n",
    "            metadata = {\"id\": res['_id']}  # Add metadata with a unique identifier\n",
    "            score = res['_score']  # Get the match score from the search result\n",
    "            documents.append((Document(page_content=json.dumps(page_content, ensure_ascii=False), metadata=metadata), score))\n",
    "        return documents\n",
    "            \n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b60505c6-e3d4-4144-9f2f-e21fb6e0de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#노리를 통해 검색한 10개 + vector 유사도 기반으로 검색한 10개 총 20개를 정렬\n",
    "def get_ensemble_results(doc_lists: List[List[Tuple[Document, float]]], weights: List[float], k: int = 5) -> List[Document]:\n",
    "        hybrid_score_dic: Dict[str, float] = {}\n",
    "        doc_map: Dict[str, Document] = {}\n",
    "        \n",
    "        # Weight-based adjustment\n",
    "        for doc_list, weight in zip(doc_lists, weights):\n",
    "            for doc, score in doc_list:\n",
    "                doc_id = doc.metadata.get(\"id\", doc.page_content)\n",
    "                if doc_id not in hybrid_score_dic:\n",
    "                    hybrid_score_dic[doc_id] = 0.0\n",
    "                hybrid_score_dic[doc_id] += score * weight\n",
    "                doc_map[doc_id] = doc\n",
    "    \n",
    "        sorted_docs = sorted(hybrid_score_dic.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [doc_map[doc_id] for doc_id, _ in sorted_docs[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6fa43228-9cce-48f4-b860-c807c5e07b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이브리드 서치 코드 -> 노리를 통해 검색한 10개 + vector 유사도 기반으로 검색한 10개 총 20개에서 rerank를 통해 최종 10개만 선택한다\n",
    "def retrieval_augmented(query , k =10): \n",
    "    \n",
    "    #embedding \n",
    "    raw_embedding = get_embedding(query)\n",
    "    #vector search 가져오기 \n",
    "    vector=vector_search(opensearch_client, raw_embedding ,k =k)\n",
    "    #lexical search 가져오기 \n",
    "    lexical=simple_text_search(opensearch_client, query ,k =k)\n",
    "\n",
    "    #rerank\n",
    "    rerank_doc = get_ensemble_results(\n",
    "            doc_lists=[vector, lexical],\n",
    "            weights= [0.7, 0.3],\n",
    "            k=k,\n",
    "    )\n",
    "    return rerank_doc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "01d4d8b4-ac1a-4500-b6ca-6afc6c0f9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#결과 검색\n",
    "result =retrieval_augmented(\"Amazon S3 GetObject 에 대해 설명해줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ffc017b8-af7d-4708-ba09-47f48967e25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 'sagemaker_doc_1017'}, page_content='{\"document\": \"하고 필요한 데이터의 하위 집합만 검색할 수 있습니다. Amazon S3 Select를 사용하여 이 데이터를 필\\\\n데이터 쿼리 적용\\\\nAPI 버전 2006-03-01 656\", \"source\": \"/home/jovyan/langtest/langraph_work_shop/data/s3-userguide.pdf\", \"page\": 674, \"total_pages\": 2716}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_1282'}, page_content='{\"document\": \"• 고객 제공 키를 사용한 서버 측 암호화(SSE-C)\\\\n• Amazon S3 콘솔에서 새 버킷을 생성할 때 기존 버킷 설정을 복사하는 옵션입니다.\\\\n디렉터리 버킷에서 지원되지 않는 Amazon S3 기능\\\\nAPI 버전 2006-03-01 766\", \"source\": \"/home/jovyan/langtest/langraph_work_shop/data/s3-userguide.pdf\", \"page\": 784, \"total_pages\": 2716}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_1366'}, page_content='{\"document\": \"작업에 대한 자세한 내용은 Amazon S3 객체에 대한 대규모 배치 작업 수행을 참조하세요.\\\\nS3 Express One Zone에서 배치 작업 사용\\\\nAPI 버전 2006-03-01 796\", \"source\": \"/home/jovyan/langtest/langraph_work_shop/data/s3-userguide.pdf\", \"page\": 814, \"total_pages\": 2716}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_668'}, page_content='{\"document\": \"\\\\\"s3:GetObjectVersion\\\\\", \\\\n지원되는 작업\\\\nAPI 버전 2006-03-01 539\", \"source\": \"/home/jovyan/langtest/langraph_work_shop/data/s3-userguide.pdf\", \"page\": 557, \"total_pages\": 2716}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_1478'}, page_content='{\"document\": \"s3express:CreateSession 권한과 s3:GetObject 권한이 있어야 합니다.\\\\n자세한 내용은 Amazon Simple Storage Service API 참조에서 CopyObject를 참조하십시오.\\\\n암호화\\\\n객체 복사\\\\nAPI 버전 2006-03-01 833\", \"source\": \"/home/jovyan/langtest/langraph_work_shop/data/s3-userguide.pdf\", \"page\": 851, \"total_pages\": 2716}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_713'}, page_content='{\"document\": \"s3:GetBucketObjectLockConfiguration 및 s3:PutObjectRetention 권한이 필요합니다. \\\\n자세한 내용은 the section called “객체 잠금 고려 사항” 섹션을 참조하세요.\\\\nREST API에서 이 작업을 사용하는 방법에 대한 자세한 내용은 Amazon Simple Storage Service API \\\\n참조의 CreateJob 작업에 있는 S3PutObjectRetention 섹션을 참조하세요.\\\\n이 작업을 사용하는 AWS Command Line Interface(AWS CLI) 예제는 the section called “객체 잠금 보\\\\n존과 함께 배치 작업 사용” 섹션을 참조하세요. AWS SDK for Java 예제는 the section called “객체 잠\\\\n금 보존과 함께 배치 작업 사용” 섹션을 참조하세요.\\\\n규제 및 제한\\\\nBatch Operations를 사용하여 Object Lock 보존 기간을 적용하는 경우 다음과 같은 규제와 제한 사항\\\\n이 적용됩니다.\\\\n• S3 Batch Operations는 버킷 수준을 변경하지 않습니다.\\\\n• 버전 관리 및 S3 객체 잠금은 작업이 수행되는 버킷에서 구성해야 합니다.\", \"source\": \"/home/jovyan/langtest/langraph_work_shop/data/s3-userguide.pdf\", \"page\": 572, \"total_pages\": 2716}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_487'}, page_content='{\"document\": \"2.\\\\n4단계에서 생성한 함수(예: serverlessrepo-ComprehendPiiRedactionS3ObjectLambda)\\\\n를 선택합니다.\\\\nS3 객체 Lambda 자습서\\\\nAPI 버전 2006-03-01 478\", \"source\": \"/home/jovyan/langtest/langraph_work_shop/data/s3-userguide.pdf\", \"page\": 496, \"total_pages\": 2716}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_976'}, page_content='{\"document\": \"니다.\\\\n인벤토리 대상 버킷은 S3 인벤토리를 설정하는 소스 버킷과 동일한 AWS 리전에 있어야 합니다. \\\\n인벤토리 대상 버킷은 다른 AWS 계정에 있을 수 있습니다.\\\\n자습서: 비디오 일괄 트랜스코딩\\\\nAPI 버전 2006-03-01 641\", \"source\": \"/home/jovyan/langtest/langraph_work_shop/data/s3-userguide.pdf\", \"page\": 659, \"total_pages\": 2716}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_818'}, page_content='{\"document\": \"작업은 지정된 Amazon S3 객체 목록에 대해 단일 작업을 수행할 수 있습니다. 단일 작업으로 엑사바\\\\nBatch Operations를 사용하여 Object Lock 관리\\\\nAPI 버전 2006-03-01 590\", \"source\": \"/home/jovyan/langtest/langraph_work_shop/data/s3-userguide.pdf\", \"page\": 608, \"total_pages\": 2716}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_260'}, page_content='{\"document\": \"다음 예제와 비슷해야 합니다.\\\\n{ \\\\n    \\\\\"ObjectLambdaAccessPointList\\\\\": [ \\\\n        { \\\\n            \\\\\"Name\\\\\": \\\\\"my-object-lambda-ap\\\\\", \\\\nAmazon S3 객체 Lambda 액세스 포인트 사용\\\\nAPI 버전 2006-03-01 402\", \"source\": \"/home/jovyan/langtest/langraph_work_shop/data/s3-userguide.pdf\", \"page\": 420, \"total_pages\": 2716}')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491dfd8c-e9f4-479a-8c1b-bc7ef5c72d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c099b5-0fed-4b8f-a76c-f06622ecba08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece2d87-0da9-465f-8b3a-ade4421d9a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fa4063-fe9c-463c-a44e-d641cc54399f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9145a-548a-4393-95a3-6b31559ca7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068fc66-e524-41de-9761-3d08093b8fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e87b4-883b-4f7c-b2c5-57654df88cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d178562-2351-4988-9ccd-c1b183663fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
