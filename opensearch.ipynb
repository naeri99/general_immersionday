{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88a6b527-1cc2-4a93-ab59-cc9397a647d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-aws (from -r requirements.txt (line 1))\n",
      "  Downloading langchain_aws-1.0.0-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting langchain (from -r requirements.txt (line 2))\n",
      "  Downloading langchain-1.0.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting langchain-core (from -r requirements.txt (line 3))\n",
      "  Downloading langchain_core-1.0.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (1.40.69)\n",
      "Collecting opensearch-py (from -r requirements.txt (line 5))\n",
      "  Downloading opensearch_py-3.0.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: pydantic in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (2.12.3)\n",
      "Collecting fitz (from -r requirements.txt (line 7))\n",
      "  Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl.metadata (816 bytes)\n",
      "Collecting sentence-transformers (from -r requirements.txt (line 8))\n",
      "  Downloading sentence_transformers-5.1.2-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting frontend (from -r requirements.txt (line 9))\n",
      "  Downloading frontend-0.0.3-py3-none-any.whl.metadata (847 bytes)\n",
      "Collecting pypdf (from -r requirements.txt (line 10))\n",
      "  Downloading pypdf-6.2.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain-aws->-r requirements.txt (line 1)) (1.26.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 6)) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 6)) (2.41.4)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 6)) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pydantic->-r requirements.txt (line 6)) (0.4.2)\n",
      "Collecting langgraph<1.1.0,>=1.0.2 (from langchain->-r requirements.txt (line 2))\n",
      "  Downloading langgraph-1.0.3-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting jsonpatch<2.0.0,>=1.33.0 (from langchain-core->-r requirements.txt (line 3))\n",
      "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langsmith<1.0.0,>=0.3.45 (from langchain-core->-r requirements.txt (line 3))\n",
      "  Downloading langsmith-0.4.42-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain-core->-r requirements.txt (line 3)) (24.2)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain-core->-r requirements.txt (line 3)) (6.0.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langchain-core->-r requirements.txt (line 3)) (9.1.2)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core->-r requirements.txt (line 3)) (3.0.0)\n",
      "Collecting langgraph-checkpoint<4.0.0,>=2.1.0 (from langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 2))\n",
      "  Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting langgraph-prebuilt<1.1.0,>=1.0.2 (from langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 2))\n",
      "  Downloading langgraph_prebuilt-1.0.2-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting langgraph-sdk<0.3.0,>=0.2.2 (from langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 2))\n",
      "  Downloading langgraph_sdk-0.2.9-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting xxhash>=3.5.0 (from langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 2))\n",
      "  Downloading xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\n",
      "Collecting ormsgpack>=1.12.0 (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 2))\n",
      "  Downloading ormsgpack-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting httpx>=0.25.2 (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 2))\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting orjson>=3.10.1 (from langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 2))\n",
      "  Downloading orjson-3.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (41 kB)\n",
      "Collecting requests-toolbelt>=1.0.0 (from langsmith<1.0.0,>=0.3.45->langchain-core->-r requirements.txt (line 3))\n",
      "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: requests>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->-r requirements.txt (line 3)) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core->-r requirements.txt (line 3)) (0.25.0)\n",
      "Requirement already satisfied: anyio in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 2)) (4.11.0)\n",
      "Requirement already satisfied: certifi in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 2)) (2025.10.5)\n",
      "Collecting httpcore==1.* (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 2))\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: idna in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 2)) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 2)) (0.16.0)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.69 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3->-r requirements.txt (line 4)) (1.40.69)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3->-r requirements.txt (line 4)) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.15.0,>=0.14.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from boto3->-r requirements.txt (line 4)) (0.14.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.41.0,>=1.40.69->boto3->-r requirements.txt (line 4)) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from botocore<1.41.0,>=1.40.69->boto3->-r requirements.txt (line 4)) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.69->boto3->-r requirements.txt (line 4)) (1.17.0)\n",
      "Collecting Events (from opensearch-py->-r requirements.txt (line 5))\n",
      "  Downloading Events-0.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core->-r requirements.txt (line 3)) (3.4.4)\n",
      "Collecting configobj (from fitz->-r requirements.txt (line 7))\n",
      "  Downloading configobj-5.0.9-py2.py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting configparser (from fitz->-r requirements.txt (line 7))\n",
      "  Downloading configparser-7.2.0-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting httplib2 (from fitz->-r requirements.txt (line 7))\n",
      "  Downloading httplib2-0.31.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting nibabel (from fitz->-r requirements.txt (line 7))\n",
      "  Downloading nibabel-5.3.2-py3-none-any.whl.metadata (9.1 kB)\n",
      "Collecting nipype (from fitz->-r requirements.txt (line 7))\n",
      "  Downloading nipype-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fitz->-r requirements.txt (line 7)) (2.3.3)\n",
      "Collecting pyxnat (from fitz->-r requirements.txt (line 7))\n",
      "  Downloading pyxnat-1.6.4-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from fitz->-r requirements.txt (line 7)) (1.15.2)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 8)) (4.67.1)\n",
      "Collecting torch>=1.11.0 (from sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 8)) (1.7.2)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading huggingface_hub-1.1.2-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sentence-transformers->-r requirements.txt (line 8)) (12.0.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 8)) (3.20.0)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 8)) (2025.11.3)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers->-r requirements.txt (line 8)) (2025.10.0)\n",
      "Collecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub>=0.20.0->sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: starlette>=0.12.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from frontend->-r requirements.txt (line 9)) (0.49.3)\n",
      "Requirement already satisfied: uvicorn>=0.7.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from frontend->-r requirements.txt (line 9)) (0.38.0)\n",
      "Requirement already satisfied: itsdangerous>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from frontend->-r requirements.txt (line 9)) (2.2.0)\n",
      "Collecting aiofiles (from frontend->-r requirements.txt (line 9))\n",
      "  Downloading aiofiles-25.1.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 2)) (1.3.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from anyio->httpx>=0.25.2->langgraph-sdk<0.3.0,>=0.2.2->langgraph<1.1.0,>=1.0.2->langchain->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: networkx in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8)) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8)) (3.1.6)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.2 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.2.0 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
      "Collecting sympy==1.13.1 (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8))\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8)) (1.3.0)\n",
      "Requirement already satisfied: click>=7.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from uvicorn>=0.7.1->frontend->-r requirements.txt (line 9)) (8.3.0)\n",
      "Requirement already satisfied: pyparsing<4,>=3.0.4 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from httplib2->fitz->-r requirements.txt (line 7)) (3.2.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 8)) (3.0.3)\n",
      "Requirement already satisfied: importlib-resources>=5.12 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from nibabel->fitz->-r requirements.txt (line 7)) (6.5.2)\n",
      "Collecting prov>=1.5.2 (from nipype->fitz->-r requirements.txt (line 7))\n",
      "  Downloading prov-2.1.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting pydot>=1.2.3 (from nipype->fitz->-r requirements.txt (line 7))\n",
      "  Downloading pydot-4.0.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting rdflib>=5.0.0 (from nipype->fitz->-r requirements.txt (line 7))\n",
      "  Downloading rdflib-7.4.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting simplejson>=3.8.0 (from nipype->fitz->-r requirements.txt (line 7))\n",
      "  Downloading simplejson-3.20.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Collecting traits>=6.2 (from nipype->fitz->-r requirements.txt (line 7))\n",
      "  Downloading traits-7.0.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.8 kB)\n",
      "Collecting acres (from nipype->fitz->-r requirements.txt (line 7))\n",
      "  Downloading acres-0.5.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting etelemetry>=0.3.1 (from nipype->fitz->-r requirements.txt (line 7))\n",
      "  Downloading etelemetry-0.3.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting looseversion!=1.2 (from nipype->fitz->-r requirements.txt (line 7))\n",
      "  Downloading looseversion-1.3.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting puremagic (from nipype->fitz->-r requirements.txt (line 7))\n",
      "  Downloading puremagic-1.30-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting ci-info>=0.2 (from etelemetry>=0.3.1->nipype->fitz->-r requirements.txt (line 7))\n",
      "  Downloading ci_info-0.3.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting isodate<1.0.0,>=0.7.2 (from rdflib>=5.0.0->nipype->fitz->-r requirements.txt (line 7))\n",
      "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->fitz->-r requirements.txt (line 7)) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->fitz->-r requirements.txt (line 7)) (2025.2)\n",
      "Collecting lxml>=4.3 (from pyxnat->fitz->-r requirements.txt (line 7))\n",
      "  Downloading lxml-6.0.2-cp310-cp310-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl.metadata (3.6 kB)\n",
      "Collecting pathlib>=1.0 (from pyxnat->fitz->-r requirements.txt (line 7))\n",
      "  Downloading pathlib-1.0.1-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 8)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 8)) (3.6.0)\n",
      "Downloading langchain_aws-1.0.0-py3-none-any.whl (150 kB)\n",
      "Downloading langchain-1.0.5-py3-none-any.whl (93 kB)\n",
      "Downloading langchain_core-1.0.4-py3-none-any.whl (471 kB)\n",
      "Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
      "Downloading langgraph-1.0.3-py3-none-any.whl (156 kB)\n",
      "Downloading langgraph_checkpoint-3.0.1-py3-none-any.whl (46 kB)\n",
      "Downloading langgraph_prebuilt-1.0.2-py3-none-any.whl (34 kB)\n",
      "Downloading langgraph_sdk-0.2.9-py3-none-any.whl (56 kB)\n",
      "Downloading langsmith-0.4.42-py3-none-any.whl (401 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Downloading opensearch_py-3.0.0-py3-none-any.whl (371 kB)\n",
      "Downloading fitz-0.0.1.dev2-py2.py3-none-any.whl (20 kB)\n",
      "Downloading sentence_transformers-5.1.2-py3-none-any.whl (488 kB)\n",
      "Downloading transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m186.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m566.1/566.1 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m197.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m201.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading frontend-0.0.3-py3-none-any.whl (32 kB)\n",
      "Downloading pypdf-6.2.0-py3-none-any.whl (326 kB)\n",
      "Downloading orjson-3.11.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (136 kB)\n",
      "Downloading ormsgpack-1.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (207 kB)\n",
      "Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
      "Downloading safetensors-0.6.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (485 kB)\n",
      "Downloading torch-2.6.0-cp310-cp310-manylinux1_x86_64.whl (766.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m766.7/766.7 MB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m  \u001b[33m0:00:04\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m267.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m145.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m91.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m  \u001b[33m0:00:08\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m  \u001b[33m0:00:02\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m57.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m61.8 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.2-py3-none-manylinux2014_x86_64.whl (150.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m150.1/150.1 MB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m  \u001b[33m0:00:03\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m6m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading triton-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (253.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.1/253.1 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m  \u001b[33m0:00:05\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\n",
      "Downloading aiofiles-25.1.0-py3-none-any.whl (14 kB)\n",
      "Downloading configobj-5.0.9-py2.py3-none-any.whl (35 kB)\n",
      "Downloading configparser-7.2.0-py3-none-any.whl (17 kB)\n",
      "Downloading Events-0.5-py3-none-any.whl (6.8 kB)\n",
      "Downloading httplib2-0.31.0-py3-none-any.whl (91 kB)\n",
      "Downloading nibabel-5.3.2-py3-none-any.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nipype-1.10.0-py3-none-any.whl (3.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading etelemetry-0.3.1-py3-none-any.whl (6.4 kB)\n",
      "Downloading ci_info-0.3.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading looseversion-1.3.0-py2.py3-none-any.whl (8.2 kB)\n",
      "Downloading prov-2.1.1-py3-none-any.whl (425 kB)\n",
      "Downloading pydot-4.0.1-py3-none-any.whl (37 kB)\n",
      "Downloading rdflib-7.4.0-py3-none-any.whl (569 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m569.0/569.0 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
      "Downloading simplejson-3.20.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
      "Downloading traits-7.0.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading acres-0.5.0-py3-none-any.whl (12 kB)\n",
      "Downloading puremagic-1.30-py3-none-any.whl (43 kB)\n",
      "Downloading pyxnat-1.6.4-py3-none-any.whl (110 kB)\n",
      "Downloading lxml-6.0.2-cp310-cp310-manylinux_2_26_x86_64.manylinux_2_28_x86_64.whl (5.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pathlib-1.0.1-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: triton, puremagic, pathlib, nvidia-cusparselt-cu12, looseversion, Events, xxhash, traits, sympy, simplejson, safetensors, pypdf, pydot, ormsgpack, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nibabel, lxml, jsonpatch, isodate, httplib2, httpcore, hf-xet, configparser, configobj, ci-info, aiofiles, acres, requests-toolbelt, rdflib, pyxnat, prov, opensearch-py, nvidia-cusparse-cu12, nvidia-cudnn-cu12, huggingface-hub, etelemetry, tokenizers, nvidia-cusolver-cu12, nipype, httpx, transformers, torch, langsmith, langgraph-sdk, frontend, fitz, sentence-transformers, langchain-core, langgraph-checkpoint, langchain-aws, langgraph-prebuilt, langgraph, langchain\n",
      "\u001b[2K  Attempting uninstall: sympy0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/62\u001b[0m [traits]cusparselt-cu12]\n",
      "\u001b[2K    Found existing installation: sympy 1.14.0━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 7/62\u001b[0m [traits]\n",
      "\u001b[2K    Uninstalling sympy-1.14.0:0m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/62\u001b[0m [sympy]\n",
      "\u001b[2K      Successfully uninstalled sympy-1.14.0━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 8/62\u001b[0m [sympy]\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62/62\u001b[0m [langchain]angchain]anggraph-prebuilt]t]]]\n",
      "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sparkmagic 0.21.0 requires pandas<2.0.0,>=0.17.1, but you have pandas 2.3.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed Events-0.5 acres-0.5.0 aiofiles-25.1.0 ci-info-0.3.0 configobj-5.0.9 configparser-7.2.0 etelemetry-0.3.1 fitz-0.0.1.dev2 frontend-0.0.3 hf-xet-1.2.0 httpcore-1.0.9 httplib2-0.31.0 httpx-0.28.1 huggingface-hub-0.36.0 isodate-0.7.2 jsonpatch-1.33 langchain-1.0.5 langchain-aws-1.0.0 langchain-core-1.0.4 langgraph-1.0.3 langgraph-checkpoint-3.0.1 langgraph-prebuilt-1.0.2 langgraph-sdk-0.2.9 langsmith-0.4.42 looseversion-1.3.0 lxml-6.0.2 nibabel-5.3.2 nipype-1.10.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-cusparselt-cu12-0.6.2 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 opensearch-py-3.0.0 orjson-3.11.4 ormsgpack-1.12.0 pathlib-1.0.1 prov-2.1.1 puremagic-1.30 pydot-4.0.1 pypdf-6.2.0 pyxnat-1.6.4 rdflib-7.4.0 requests-toolbelt-1.0.0 safetensors-0.6.2 sentence-transformers-5.1.2 simplejson-3.20.2 sympy-1.13.1 tokenizers-0.22.1 torch-2.6.0 traits-7.0.2 transformers-4.57.1 triton-3.2.0 xxhash-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2347ea6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n"
     ]
    }
   ],
   "source": [
    "from langchain_aws import ChatBedrockConverse\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langchain_core.prompts import BasePromptTemplate\n",
    "from langchain_core.output_parsers import BaseOutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "import os\n",
    "import boto3\n",
    "import json\n",
    "import sys\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "from langchain_core.documents import Document\n",
    "from pypdf import PdfReader\n",
    "import concurrent.futures\n",
    "from langchain_aws import BedrockEmbeddings\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from pydantic import BaseModel, Field\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47fa1678-f7f8-4524-b1ef-9a604cae1061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89922941d7dd4056931a3a2573566682",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0427b92570db43f09d236506fae8369c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3742adf3519541a4a8a308ab194595af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bd9da4381f54788a80c32a8b5e92388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3925995228224fd0bec543c67223c16d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49da270636e84b459c029ca26aca3fd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10fc68c6f33c47f2895968c78fcafdcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3af074df7368498dae1d77879d74884e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74b17cc4d85948319cac1d9e0f640241",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f38137d82814012940afbd4a65bab79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224a7f5baa284385b97b0e7c4c6db4b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab272cd2-8f12-4ccd-aec2-9265f27468e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#text를 vector로 변환\n",
    "def get_embedding(text):\n",
    "    embeddings = embedding_model.encode(text)\n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34406be2-aa1a-4e2e-b9a5-003354306ab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "384"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(get_embedding(\"안녕하세요?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14c69f1e-8e6d-4bff-9df0-f5cbc481c4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opensearch에 index(sql table과 유사) 생성\n",
    "def define_index(opensearch_client, index_name):\n",
    "    index_settings = {\n",
    "        \"settings\": {\n",
    "            \"index\": {  \n",
    "                \"knn\": True,                          \n",
    "                \"knn.algo_param.ef_search\": 100,      \n",
    "                \"number_of_shards\": 3,                \n",
    "                \"number_of_replicas\": 2, \n",
    "                \"analysis\": {\n",
    "                    \"analyzer\": {\n",
    "                        \"nori_analyzer\": {\n",
    "                            \"tokenizer\": \"nori_tokenizer\",\n",
    "                            \"filter\": [\"nori_stop\", \"lowercase\"]\n",
    "                        }\n",
    "                    },\n",
    "                    \"filter\": {\n",
    "                        \"nori_stop\": {\n",
    "                            \"type\": \"nori_part_of_speech\",\n",
    "                            \"stoptags\": [\"J\", \"JKS\", \"JKB\", \"JKO\", \"JKG\", \"JKC\", \"JKV\", \"JKQ\", \"JX\", \"JC\"]\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },   \n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"document\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"analyzer\": \"nori_analyzer\"\n",
    "                },\n",
    "                \"doc_vector\": {\n",
    "                    \"type\": \"knn_vector\",\n",
    "                    \"dimension\": 384,\n",
    "                    \"method\": {\n",
    "                        \"name\": \"hnsw\",   \n",
    "                        \"space_type\": \"l2\",\n",
    "                        \"engine\": \"faiss\",\n",
    "                        \"parameters\": {\n",
    "                            \"ef_construction\": 128,\n",
    "                            \"m\": 16\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"source\": {\n",
    "                    \"type\": \"keyword\"  # Use keyword for exact matching\n",
    "                },\n",
    "                \"page\": {\n",
    "                    \"type\": \"integer\"\n",
    "                },\n",
    "                \"total_pages\": {\n",
    "                    \"type\": \"integer\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    try:\n",
    "        response = opensearch_client.indices.create(\n",
    "            index=index_name,\n",
    "            body=index_settings\n",
    "        )\n",
    "        print(\"Index created successfully:\", response)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(\"Error creating index:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b20520f7-34e9-4aea-88fb-59b55c45ee9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf파일 opensearch에 적재\n",
    "def save_chunk(opensearh_client,  index_name,chunks):\n",
    "    for d in enumerate(chunks):\n",
    "        document = d[1].page_content\n",
    "        raw_embedding = get_embedding(d[1].page_content)\n",
    "        doc_vector = [float(val) for val in raw_embedding]\n",
    "        source = d[1].metadata[\"source\"]\n",
    "        page_info= d[1].metadata[\"page\"]\n",
    "        total_page=d[1].metadata[\"total_pages\"]\n",
    "    \n",
    "        # Prepare document for indexing\n",
    "        document = {\n",
    "            \"document\": document,\n",
    "            \"doc_vector\":doc_vector,\n",
    "            \"source\" : source, \n",
    "            \"page\" : page_info , \n",
    "            \"total_pages\" : total_page \n",
    "        }\n",
    "        try:\n",
    "            response = opensearh_client.index(\n",
    "                index=index_name,\n",
    "                body=document,\n",
    "                id=f\"sagemaker_doc_{d[0]}\"\n",
    "            )\n",
    "            if d[0] % 500 ==0 :\n",
    "                print(f\"Document {d[0]} indexed successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error indexing document {d[0]}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e74069b2-4be4-4fb4-b5a8-b3cf58efb5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#속도를 위해 parallel하게 진행. 페이지를 부분적으로 나누어서 text파일 읽기\n",
    "def process_page_range(pdf_path, start_page, end_page):\n",
    "    \"\"\"Process a range of pages from a PDF.\"\"\"\n",
    "    # Open the PDF\n",
    "    doc = PdfReader(pdf_path)\n",
    "    total_pages = len(doc.pages)  # Fix: use len(doc.pages)\n",
    "    \n",
    "    batch_docs = []\n",
    "    for i in range(start_page, end_page):\n",
    "        if i >= total_pages:\n",
    "            break\n",
    "            \n",
    "        # Get page and extract text\n",
    "        page = doc.pages[i]  # Fix: use doc.pages[i]\n",
    "        text = page.extract_text()  # Fix: use extract_text()\n",
    "        \n",
    "        # Create Document object\n",
    "        doc_obj = Document(\n",
    "            page_content=text,\n",
    "            metadata={\n",
    "                \"source\": pdf_path,\n",
    "                \"page\": i + 1,\n",
    "                \"total_pages\": total_pages\n",
    "            }\n",
    "        )\n",
    "        batch_docs.append(doc_obj)\n",
    "    \n",
    "    return batch_docs\n",
    "    \n",
    "#pdf파일 읽기 \n",
    "def load_pdf_parallel(pdf_path, batch_size=100, max_workers=4):\n",
    "    \"\"\"Load a PDF using parallel processing.\"\"\"\n",
    "    # Get total pages\n",
    "    doc = PdfReader(pdf_path)\n",
    "    total_pages = len(doc.pages)  # Fix: use len(doc.pages)\n",
    "    # Remove doc.close() - not needed for pypdf\n",
    "    print(f\"PDF has {total_pages} pages in total\")\n",
    "    \n",
    "    # Create batches\n",
    "    batches = []\n",
    "    for start_page in range(0, total_pages, batch_size):\n",
    "        end_page = min(start_page + batch_size, total_pages)\n",
    "        batches.append((start_page, end_page))\n",
    "    \n",
    "    all_docs = []\n",
    "    \n",
    "    with concurrent.futures.ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Submit tasks\n",
    "        future_to_batch = {\n",
    "            executor.submit(process_page_range, pdf_path, start, end): (start, end)\n",
    "            for start, end in batches\n",
    "        }\n",
    "        \n",
    "        # Process results as they complete\n",
    "        for future in concurrent.futures.as_completed(future_to_batch):\n",
    "            start, end = future_to_batch[future]\n",
    "            try:\n",
    "                batch_docs = future.result()\n",
    "                all_docs.extend(batch_docs)\n",
    "                print(f\"Completed batch: pages {start+1} to {end}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing pages {start+1} to {end}: {e}\")\n",
    "    \n",
    "    # Sort by page number\n",
    "    all_docs.sort(key=lambda x: x.metadata[\"page\"])\n",
    "    \n",
    "    return all_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "110a12f5-b02e-4170-94a1-b90e793889f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected to OpenSearch\n",
      "{'name': '7dd5ae15fb2e9bf062b1cf97e77abe59', 'cluster_name': '010928186392:bnk-opensearch-domain', 'cluster_uuid': '55wiqk1PT-yFRCGPF6bwcw', 'version': {'number': '7.10.2', 'build_type': 'tar', 'build_hash': 'unknown', 'build_date': '2025-08-01T07:59:18.205170174Z', 'build_snapshot': False, 'lucene_version': '9.11.1', 'minimum_wire_compatibility_version': '7.10.0', 'minimum_index_compatibility_version': '7.0.0'}, 'tagline': 'The OpenSearch Project: https://opensearch.org/'}\n"
     ]
    }
   ],
   "source": [
    "#opensearh client 연결 생성 및 확인\n",
    "opensearch_client = OpenSearch(\n",
    "    hosts = [{'host': 'search-bnk-opensearch-domain-lmbhroa2z3ptbpsai3mf45jha4.ap-northeast-2.es.amazonaws.com', 'port': 443}],\n",
    "    http_auth = ('Bnkadmin', 'Bnkadmin123!'),  # Replace with your master credentials\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "try:\n",
    "    response = opensearch_client.info()\n",
    "    print(\"Successfully connected to OpenSearch\")\n",
    "    print(response)\n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "72cb09a9-cd18-44f0-82cc-9b1b18bf1601",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"nori_test_index\"\n",
    "\n",
    "index_body = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                \"nori_analyzer\": {\n",
    "                    \"type\": \"nori\",\n",
    "                    \"decompound_mode\": \"mixed\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"title\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"nori_analyzer\"\n",
    "            },\n",
    "            \"content\": {\n",
    "                \"type\": \"text\",\n",
    "                \"analyzer\": \"nori_analyzer\"\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9108c96c-eb04-412d-a55a-bd768bd29d98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 's3_explain' deleted successfully: {'acknowledged': True}\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    delete_response = opensearch_client.indices.delete(index=index_name)\n",
    "    print(f\"Index 's3_explain' deleted successfully: {delete_response}\")\n",
    "except Exception as delete_error:\n",
    "    print(f\"Failed to delete index 's3_explain': {delete_error}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3b8933ca-949e-42d3-8756-507baa821ad6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True, 'shards_acknowledged': True, 'index': 'nori_test_index'}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 인덱스 생성\n",
    "\n",
    "opensearch_client.indices.create(index=index_name, body=index_body)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8cd82b28-8b12-44eb-8e94-38b10d7c0b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 테스트 문서 인덱싱\n",
    "test_docs = [\n",
    "    {\"title\": \"한국어 형태소 분석\", \"content\": \"노리 분석기는 한국어를 잘 처리합니다\"},\n",
    "    {\"title\": \"은행 업무 시스템\", \"content\": \"금융권에서 사용하는 검색 시스템입니다\"},\n",
    "    {\"title\": \"OpenSearch 활용\", \"content\": \"엘라스틱서치 호환 검색엔진입니다\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "55c0ce95-d35a-4d3c-8184-3c21d47de5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, doc in enumerate(test_docs):\n",
    "    opensearch_client.index(index=index_name, id=i+1, body=doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65aa5238-d71a-46b5-9685-b674c1793014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "문서 1: 한국어 형태소 분석\n",
      "원문: 노리 분석기는 한국어를 잘 처리합니다\n",
      "분석된 토큰: 노리 | 분석기 | 분석 | 기 | 한국어 | 한국 | 어 | 처리\n",
      "\n",
      "문서 2: 은행 업무 시스템\n",
      "원문: 금융권에서 사용하는 검색 시스템입니다\n",
      "분석된 토큰: 금융 | 사용 | 검색 | 시스템 | 입니다 | 이\n",
      "\n",
      "문서 3: OpenSearch 활용\n",
      "원문: 엘라스틱서치 호환 검색엔진입니다\n",
      "분석된 토큰: 엘라스틱 | 서치 | 호환 | 검색 | 엔진 | 입니다 | 이\n"
     ]
    }
   ],
   "source": [
    "for i, doc in enumerate(test_docs):\n",
    "    print(f\"\\n문서 {i+1}: {doc['title']}\")\n",
    "    print(f\"원문: {doc['content']}\")\n",
    "    \n",
    "    # 노리 analyzer로 분석\n",
    "    analyze_result = opensearch_client.indices.analyze(\n",
    "        index=index_name,\n",
    "        body={\n",
    "            \"analyzer\": \"nori_analyzer\",\n",
    "            \"text\": doc['content']\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    tokens = [token['token'] for token in analyze_result['tokens']]\n",
    "    print(f\"분석된 토큰: {' | '.join(tokens)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c837db0f-728d-4d83-9a0c-970c7bbd2298",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "검색 결과: 0건\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 4. 검색 테스트\n",
    "search_query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"content\": \"한국어\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "search_result = opensearch_client.search(index=index_name, body=search_query)\n",
    "print(f\"\\n검색 결과: {search_result['hits']['total']['value']}건\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8935ce5c-d4c7-4ef4-9463-e23aaac8cf43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fe6fcd42-95f0-48ef-ae4b-fc53d297db0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF has 3031 pages in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch: pages 401 to 500\n",
      "Completed batch: pages 501 to 600\n",
      "Completed batch: pages 601 to 700\n",
      "Completed batch: pages 301 to 400\n",
      "Completed batch: pages 701 to 800\n",
      "Completed batch: pages 201 to 300\n",
      "Completed batch: pages 101 to 200\n",
      "Completed batch: pages 801 to 900\n",
      "Completed batch: pages 1001 to 1100\n",
      "Completed batch: pages 901 to 1000\n",
      "Completed batch: pages 1101 to 1200\n",
      "Completed batch: pages 1201 to 1300\n",
      "Completed batch: pages 1401 to 1500\n",
      "Completed batch: pages 1301 to 1400\n",
      "Completed batch: pages 1 to 100\n",
      "Completed batch: pages 1501 to 1600\n",
      "Completed batch: pages 1601 to 1700\n",
      "Completed batch: pages 1901 to 2000\n",
      "Completed batch: pages 1801 to 1900\n",
      "Completed batch: pages 2001 to 2100\n",
      "Completed batch: pages 2101 to 2200\n",
      "Completed batch: pages 1701 to 1800\n",
      "Completed batch: pages 2201 to 2300\n",
      "Completed batch: pages 2401 to 2500\n",
      "Completed batch: pages 2301 to 2400\n",
      "Completed batch: pages 2501 to 2600\n",
      "Completed batch: pages 2701 to 2800\n",
      "Completed batch: pages 2801 to 2900\n",
      "Completed batch: pages 2901 to 3000\n",
      "Completed batch: pages 2601 to 2700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed batch: pages 3001 to 3031\n"
     ]
    }
   ],
   "source": [
    "#pdf파일을 chunk로 나눈다\n",
    "path=\"./s3-userguide.pdf\"\n",
    "page=load_pdf_parallel(path,max_workers=os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e61325a0-2acb-4546-99fb-51d8d841767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#반환된 결과는 List[Document] 형태입니다.\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=100)\n",
    "split_docs = text_splitter.split_documents(page)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6cbf2f30-d2eb-464d-b566-b615a15d8cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 갯수 ->  8170\n"
     ]
    }
   ],
   "source": [
    "print(\"총 갯수 -> \",len(split_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7b75f974-2a47-4ad5-850c-e3c5209ccd0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to delete index 's3_explain': NotFoundError(404, 'index_not_found_exception', 'no such index [s3_explain]', s3_explain, index_or_alias)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    delete_response = opensearch_client.indices.delete(index='s3_explain')\n",
    "    print(f\"Index 's3_explain' deleted successfully: {delete_response}\")\n",
    "except Exception as delete_error:\n",
    "    print(f\"Failed to delete index 's3_explain': {delete_error}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Connection failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8b9e93b2-0c34-4530-8788-13581dbc4991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index created successfully: {'acknowledged': True, 'shards_acknowledged': True, 'index': 's3_explain'}\n"
     ]
    }
   ],
   "source": [
    "#index 생성\n",
    "define_index(opensearch_client, \"s3_explain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "20a5f715-ffbb-4961-a667-65dfef4accfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': './s3-userguide.pdf', 'page': 340, 'total_pages': 3031}, page_content='때까지 업로드가 진행 중인 것으로 간주됩니다.\\n멀티파트 업로드를 중지하려면 다음을 수행할 수 있습니다.\\n1 S3Client 인스턴스를 만듭니다.\\n2 버킷 이름과 기타 필수 파라미터를 전달하여 클라이언트의 중단 메서드를 사용합니\\n다.\\nNote\\n특정 멀티파트 업로드를 중지할 수도 있습니다. 자세한 내용은 AWS SDK 사용(하위 수준 \\nAPI) 섹션을 참조하세요.\\nAWS SDK for Java를 사용하여 멀티파트 업로드를 중단하는 방법의 예는 Amazon S3 API 참조의\\nCancel a multipart upload를 참조하세요.\\n멀티파트 업로드 사용 API 버전 2006-03-01 319'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 341, 'total_pages': 3031}, page_content='Amazon Simple Storage Service 사용 설명서\\n.NET\\n다음 C# 예제는 1주일 이상 전에 특정 버킷에서 시작되어 진행 중인 멀티파트 업로드를 모두 중지\\n합니다. 코드 예제 설정 및 실행에 대한 자세한 내용은 AWS SDK for .NET 개발자 안내서의 AWS \\nSDK for .NET 시작하기를 참조하세요.\\nusing Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Transfer;\\nusing System;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n{ \\n    class AbortMPUUsingHighLevelAPITest \\n    { \\n        private const string bucketName = \"*** provide bucket name ***\"; \\n        // Specify your bucket region (an example region is shown). \\n        private static readonly RegionEndpoint bucketRegion = \\n RegionEndpoint.USWest2;'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 341, 'total_pages': 3031}, page_content='private static readonly RegionEndpoint bucketRegion = \\n RegionEndpoint.USWest2; \\n        private static IAmazonS3 s3Client; \\n        public static void Main() \\n        { \\n            s3Client = new AmazonS3Client(bucketRegion); \\n            AbortMPUAsync().Wait(); \\n        } \\n        private static async Task AbortMPUAsync() \\n        { \\n            try \\n            { \\n                var transferUtility = new TransferUtility(s3Client); \\n                // Abort all in-progress uploads initiated before the specified \\n date.'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 341, 'total_pages': 3031}, page_content='// Abort all in-progress uploads initiated before the specified \\n date. \\n                await transferUtility.AbortMultipartUploadsAsync( \\n                    bucketName, DateTime.Now.AddDays(-7)); \\n            } \\n            catch (AmazonS3Exception e) \\n            { \\n                Console.WriteLine(\"Error encountered on server. Message:\\'{0}\\' when \\n writing an object\", e.Message); \\n멀티파트 업로드 사용 API 버전 2006-03-01 320'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 342, 'total_pages': 3031}, page_content='Amazon Simple Storage Service 사용 설명서\\n            } \\n            catch (Exception e) \\n            { \\n                Console.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when \\n writing an object\", e.Message); \\n            } \\n        }  \\n    }\\n}\\nNote\\n특정 멀티파트 업로드를 중지할 수도 있습니다. 자세한 내용은 AWS SDK 사용(하위 수준 \\nAPI) 섹션을 참조하세요.\\nAWS SDK 사용(하위 수준 API)\\nAmazonS3.abortMultipartUpload 메서드를 호출하여 진행 중인 멀티파트 업로드를 중단할 수 있\\n습니다. 이 메서드는 Amazon S3에 업로드된 모든 부분을 삭제하므로 가용 리소스가 늘어나게 됩니다. \\n업로드 ID, 버킷 이름 및 키 이름을 제공해야 합니다. 다음은 진행 중인 멀티파트 업로드를 중지하는 방\\n법을 보여주는 Java 코드 예제입니다.'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 342, 'total_pages': 3031}, page_content='업로드 ID, 버킷 이름 및 키 이름을 제공해야 합니다. 다음은 진행 중인 멀티파트 업로드를 중지하는 방\\n법을 보여주는 Java 코드 예제입니다.\\n멀티파트 업로드를 중지하려면 업로드에 사용된 업로드 ID, 버킷 및 키 이름을 제공해야 합니다. 멀티\\n파트 업로드를 중지하면 해당 업로드 ID를 사용해 추가 파트를 업로드할 수 없습니다. Amazon S3 멀\\n티파트 업로드에 대한 자세한 내용은 Amazon S3에서 멀티파트 업로드를 사용한 객체 업로드 및 복사\\n섹션을 참조하세요.\\nJava\\nAWS SDK for Java를 사용하여 진행 중인 특정 멀티파트 업로드를 중지하려면 하위 수준 API를 사\\n용하여 버킷 이름, 객체 키 및 업로드 ID를 제공함으로써 업로드를 중단할 수 있습니다.\\nNote\\n특정 멀티파트 업로드를 중단하는 대신, 특정 시간 이전에 시작되어 아직 진행 중인 모든 멀\\n티파트 업로드를 중지할 수 있습니다. 이 정리 작업은 시작되었지만 완료되거나 중지되지 \\n않은 이전 멀티파트 업로드를 중지하는 데 유용합니다. 자세한 내용은 AWS SDK 사용(상위 \\n수준 API) 섹션을 참조하세요.\\n멀티파트 업로드 사용 API 버전 2006-03-01 321'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 343, 'total_pages': 3031}, page_content='Amazon Simple Storage Service 사용 설명서\\nAWS SDK for Java를 사용하여 특정 멀티파트 업로드를 중단하는 방법의 예는 Amazon S3 API 참\\n조의 Cancel a multipart upload를 참조하세요.\\n.NET\\n다음 C# 예제는 멀티파트 업로드를 중지하는 방법을 보여줍니다. 다음 코드가 포함된 전체 C# 예제\\n는 AWS SDK 사용(하위 수준 API) 섹션을 참조하세요.\\nAbortMultipartUploadRequest abortMPURequest = new AbortMultipartUploadRequest\\n{ \\n    BucketName = existingBucketName, \\n    Key = keyName, \\n    UploadId = initResponse.UploadId\\n};\\nawait AmazonS3Client.AbortMultipartUploadAsync(abortMPURequest);\\n또한 특정 시간 이전에 시작되어 진행 중인 멀티파트 업로드를 중단할 수도 있습니다. 이 정리 작업\\n은 완료 또는 중단되지 않은 멀티파트 업로드를 중단할 때 유용합니다. 자세한 내용은 AWS SDK 사'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 343, 'total_pages': 3031}, page_content=\"은 완료 또는 중단되지 않은 멀티파트 업로드를 중단할 때 유용합니다. 자세한 내용은 AWS SDK 사\\n용(상위 수준 API) 섹션을 참조하세요.\\nPHP\\n이 예제에서는 진행 중인 멀티파트 업로드를 중지하기 위해 AWS SDK for PHP 버전 3의 클래스를 \\n사용하는 방법을 보여줍니다. AWS SDK for Ruby API에 대한 자세한 내용은 AWS SDK for Ruby – \\n버전 2를 참조하세요. 예를 들어 abortMultipartUpload() 메서드입니다.\\nAWS SDK for Ruby API에 대한 자세한 내용은 AWS SDK for Ruby – 버전 2를 참조하세요.\\n require 'vendor/autoload.php';\\nuse Aws\\\\S3\\\\S3Client;\\n$bucket = '*** Your Bucket Name ***';\\n$keyname = '*** Your Object Key ***';\\n$uploadId = '*** Upload ID of upload to Abort ***';\\n$s3 = new S3Client([ \\n    'version' => 'latest', \\n    'region'  => 'us-east-1'\\n]);\"),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 343, 'total_pages': 3031}, page_content=\"$s3 = new S3Client([ \\n    'version' => 'latest', \\n    'region'  => 'us-east-1'\\n]);\\n// Abort the multipart upload.\\n$s3->abortMultipartUpload([ \\n멀티파트 업로드 사용 API 버전 2006-03-01 322\"),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 344, 'total_pages': 3031}, page_content=\"Amazon Simple Storage Service 사용 설명서\\n    'Bucket'   => $bucket, \\n    'Key'      => $keyname, \\n    'UploadId' => $uploadId,\\n]);\\n멀티파트 업로드를 사용한 객체 복사\\n멀티파트 업로드를 통해 객체를 여러 부분의 집합으로 복사할 수 있습니다. 이 섹션의 예제는 멀티파트 \\n업로드 API를 사용하여 5GB보다 큰 객체를 복사하는 방법을 보여 줍니다. 멀티파트 업로드에 대한 내\\n용은 Amazon S3에서 멀티파트 업로드를 사용한 객체 업로드 및 복사 섹션을 참조하세요.\\n멀티파트 업로드 API를 사용하지 않고 단일 작업으로는 5GB보다 작은 객체를 복사할 수 있습니다. \\nAWS Management Console, AWS CLI, REST API 또는 AWS SDK를 사용하여 5GB보다 작은 객체를 \\n복사할 수 있습니다. 자세한 내용은 객체 복사, 이동, 이름 변경 섹션을 참조하세요.\\n추가 체크섬이 포함된 멀티파트 업로드를 사용하여 객체를 업로드하는 전체 절차는 튜토리얼: 멀티파\\n트 업로드를 통한 객체 업로드 및 데이터 무결성 확인 섹션을 참조하세요.\"),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 344, 'total_pages': 3031}, page_content='추가 체크섬이 포함된 멀티파트 업로드를 사용하여 객체를 업로드하는 전체 절차는 튜토리얼: 멀티파\\n트 업로드를 통한 객체 업로드 및 데이터 무결성 확인 섹션을 참조하세요.\\n다음 섹션에서는 REST API 또는 AWS SDK를 사용하여 멀티파트 업로드로 객체를 복사하는 방법을 \\n보여줍니다.\\nREST API 사용\\nAmazon Simple Storage Service API 참조의 다음 섹션에서는 멀티파트 업로드를 위한 REST API에 \\n대해 설명합니다. 기존 객체를 복사하려면, 파트 업로드(복사) API를 사용하고 요청에 x-amz-copy-\\nsource 요청 헤더를 추가하여 소스 객체를 지정합니다.\\n• 멀티파트 업로드 시작\\n• 부분 업로드\\n• 부분 업로드(복사)\\n• 멀티파트 업로드 완료\\n• 멀티파트 업로드 중단\\n• 부분 목록 조회\\n• 멀티파트 업로드 목록 조회\\n이러한 API를 사용하여 REST 요청을 만들거나, 제공된 SDK 중 하나를 사용할 수 있습니다. AWS CLI\\n에서 멀티파트 업로드를 사용하는 방법에 대한 자세한 내용은 AWS CLI 사용 단원을 참조하십시오. \\nSDK에 대한 자세한 내용은 AWS멀티파트 업로드에 대한 SDK 지원을 참조하십시오.'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 344, 'total_pages': 3031}, page_content='SDK에 대한 자세한 내용은 AWS멀티파트 업로드에 대한 SDK 지원을 참조하십시오.\\n멀티파트 업로드 사용 API 버전 2006-03-01 323'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 345, 'total_pages': 3031}, page_content='Amazon Simple Storage Service 사용 설명서\\nAWS SDK 사용\\n하위 수준 API를 사용하여 객체를 복사하려면 다음을 수행합니다.\\n• AmazonS3Client.initiateMultipartUpload() 메서드를 호출하여 멀티파트 업로드를 시작\\n합니다.\\n• AmazonS3Client.initiateMultipartUpload() 메서드가 반환하는 응답 객체에서 업로드 ID\\n를 저장합니다. 이후의 각 파트 업로드 작업에서 이 업로드 ID를 제공합니다.\\n• 모든 파트를 복사합니다. 복사해야 하는 각 파트에 대해 CopyPartRequest 클래스의 새 인스턴스\\n를 생성합니다. 소스 및 대상 버킷 이름, 소스 및 대상 객체 키, 업로드 ID, 파트의 첫 번째 및 마지막 \\n바이트 위치와 파트 번호 등과 같은 파트 정보를 제공합니다.\\n• AmazonS3Client.copyPart() 메서드 호출에 대한 응답을 저장합니다. 각 응답에는 업로드된 파\\n트의 파트 번호화 ETag 값이 포함되어 있습니다. 멀티파트 업로드를 완료하려면 이러한 정보가 필\\n요합니다.'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 345, 'total_pages': 3031}, page_content='트의 파트 번호화 ETag 값이 포함되어 있습니다. 멀티파트 업로드를 완료하려면 이러한 정보가 필\\n요합니다.\\n• AmazonS3Client.completeMultipartUpload() 메서드를 호출하여 복사 작업을 완료합니다.\\nJava\\nAWS SDK for Java를 통해 멀티파트 업로드를 사용하여 객체를 복사하는 방법의 예는 Amazon S3 \\nAPI 참조의 Copy part of an object from another object를 참조하세요.\\n.NET\\n다음 C# 예제는 SDK for .NET을 사용하여 5GB보다 큰 Amazon S3 객체를 하나의 소스 위치에서 \\n다른 위치(예: 한 버킷에서 다른 버킷으로)로 복사합니다. 5GB보다 작은 객체를 복사하려면 AWS \\nSDK 사용에 설명된 단일 작업 복사 프로시저를 사용하십시오. Amazon S3 멀티파트 업로드에 대\\n한 자세한 내용은 Amazon S3에서 멀티파트 업로드를 사용한 객체 업로드 및 복사 섹션을 참조하세\\n요.\\n이 예제에서는 SDK for .NET 멀티파트 업로드 API를 사용하여 S3 버킷 간에 5GB보다 큰 Amazon \\nS3 객체를 복사하는 방법을 보여 줍니다.'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 345, 'total_pages': 3031}, page_content='요.\\n이 예제에서는 SDK for .NET 멀티파트 업로드 API를 사용하여 S3 버킷 간에 5GB보다 큰 Amazon \\nS3 객체를 복사하는 방법을 보여 줍니다.\\nusing Amazon;\\nusing Amazon.S3;\\nusing Amazon.S3.Model;\\nusing System;\\nusing System.Collections.Generic;\\nusing System.Threading.Tasks;\\nnamespace Amazon.DocSamples.S3\\n멀티파트 업로드 사용 API 버전 2006-03-01 324'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 346, 'total_pages': 3031}, page_content='Amazon Simple Storage Service 사용 설명서\\n{ \\n    class CopyObjectUsingMPUapiTest \\n    { \\n        private const string sourceBucket = \"*** provide the name of the bucket with \\n source object ***\"; \\n        private const string targetBucket = \"*** provide the name of the bucket to \\n copy the object to ***\"; \\n        private const string sourceObjectKey = \"*** provide the name of object to \\n copy ***\"; \\n        private const string targetObjectKey = \"*** provide the name of the object \\n copy ***\"; \\n        // Specify your bucket region (an example region is shown).'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 346, 'total_pages': 3031}, page_content='copy ***\"; \\n        // Specify your bucket region (an example region is shown). \\n        private static readonly RegionEndpoint bucketRegion = \\n RegionEndpoint.USWest2;  \\n        private static IAmazonS3 s3Client; \\n        public static void Main() \\n        { \\n            s3Client = new AmazonS3Client(bucketRegion); \\n            Console.WriteLine(\"Copying an object\"); \\n            MPUCopyObjectAsync().Wait(); \\n        } \\n        private static async Task MPUCopyObjectAsync() \\n        { \\n            // Create a list to store the upload part responses.'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 346, 'total_pages': 3031}, page_content='{ \\n            // Create a list to store the upload part responses. \\n            List<UploadPartResponse> uploadResponses = new \\n List<UploadPartResponse>(); \\n            List<CopyPartResponse> copyResponses = new List<CopyPartResponse>(); \\n            // Setup information required to initiate the multipart upload. \\n            InitiateMultipartUploadRequest initiateRequest = \\n                new InitiateMultipartUploadRequest \\n                { \\n                    BucketName = targetBucket, \\n                    Key = targetObjectKey \\n                };'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 346, 'total_pages': 3031}, page_content='Key = targetObjectKey \\n                }; \\n            // Initiate the upload. \\n            InitiateMultipartUploadResponse initResponse = \\n                await s3Client.InitiateMultipartUploadAsync(initiateRequest); \\n            // Save the upload ID. \\n            String uploadId = initResponse.UploadId; \\n멀티파트 업로드 사용 API 버전 2006-03-01 325'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 347, 'total_pages': 3031}, page_content='Amazon Simple Storage Service 사용 설명서\\n            try \\n            { \\n                // Get the size of the object. \\n                GetObjectMetadataRequest metadataRequest = new \\n GetObjectMetadataRequest \\n                { \\n                    BucketName = sourceBucket, \\n                    Key = sourceObjectKey \\n                }; \\n                GetObjectMetadataResponse metadataResponse = \\n                    await s3Client.GetObjectMetadataAsync(metadataRequest); \\n                long objectSize = metadataResponse.ContentLength; // Length in \\n bytes.'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 347, 'total_pages': 3031}, page_content='long objectSize = metadataResponse.ContentLength; // Length in \\n bytes. \\n                // Copy the parts. \\n                long partSize = 5 * (long)Math.Pow(2, 20); // Part size is 5 MB. \\n                long bytePosition = 0; \\n                for (int i = 1; bytePosition < objectSize; i++) \\n                { \\n                    CopyPartRequest copyRequest = new CopyPartRequest \\n                    { \\n                        DestinationBucket = targetBucket, \\n                        DestinationKey = targetObjectKey, \\n                        SourceBucket = sourceBucket,'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 347, 'total_pages': 3031}, page_content='SourceBucket = sourceBucket, \\n                        SourceKey = sourceObjectKey, \\n                        UploadId = uploadId, \\n                        FirstByte = bytePosition, \\n                        LastByte = bytePosition + partSize - 1 >= objectSize ? \\n objectSize - 1 : bytePosition + partSize - 1, \\n                        PartNumber = i \\n                    }; \\n                    copyResponses.Add(await s3Client.CopyPartAsync(copyRequest)); \\n                    bytePosition += partSize; \\n                } \\n                // Set up to complete the copy.'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 347, 'total_pages': 3031}, page_content='} \\n                // Set up to complete the copy. \\n                CompleteMultipartUploadRequest completeRequest = \\n                new CompleteMultipartUploadRequest \\n                { \\n                    BucketName = targetBucket, \\n멀티파트 업로드 사용 API 버전 2006-03-01 326'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 348, 'total_pages': 3031}, page_content='Amazon Simple Storage Service 사용 설명서\\n                    Key = targetObjectKey, \\n                    UploadId = initResponse.UploadId \\n                }; \\n                completeRequest.AddPartETags(copyResponses); \\n                // Complete the copy. \\n                CompleteMultipartUploadResponse completeUploadResponse =  \\n                    await s3Client.CompleteMultipartUploadAsync(completeRequest); \\n            } \\n            catch (AmazonS3Exception e) \\n            { \\n                Console.WriteLine(\"Error encountered on server. Message:\\'{0}\\' when'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 348, 'total_pages': 3031}, page_content='{ \\n                Console.WriteLine(\"Error encountered on server. Message:\\'{0}\\' when \\n writing an object\", e.Message); \\n            } \\n            catch (Exception e) \\n            { \\n                Console.WriteLine(\"Unknown encountered on server. Message:\\'{0}\\' when \\n writing an object\", e.Message); \\n            } \\n        } \\n    }\\n}\\n튜토리얼: 멀티파트 업로드를 통한 객체 업로드 및 데이터 무결성 확인\\n멀티파트 업로드를 사용하면 단일 객체를 여러 부분의 집합으로 업로드할 수 있습니다. 각 부분은 객체 \\n데이터의 연속적인 부분입니다. 이러한 객체 부분은 독립적으로 그리고 임의의 순서로 업로드할 수 있\\n습니다. 부분의 전송이 실패할 경우 다른 부분에 영향을 주지 않고도 해당 부분을 재전송할 수 있습니'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 348, 'total_pages': 3031}, page_content='습니다. 부분의 전송이 실패할 경우 다른 부분에 영향을 주지 않고도 해당 부분을 재전송할 수 있습니\\n다. 객체의 모든 부분이 업로드되면 Amazon S3가 이들 부분을 수집하여 객체를 생성합니다. 일반적으\\n로 객체 크기가 100MB에 근접할 경우, 단일 작업에서 객체를 업로드하는 대신 멀티파트 업로드 사용\\n을 고려해 봐야 합니다. 멀티파트 업로드에 대한 자세한 내용은 Amazon S3에서 멀티파트 업로드를 사\\n용한 객체 업로드 및 복사 섹션을 참조하십시오. 멀티파트 업로드와 관련된 제한은 Amazon S3 멀티파\\n트 업로드 제한 섹션을 참조하세요.\\n체크섬을 사용하여 자산을 복사할 때 자산이 변경되지 않았는지 확인할 수 있습니다. 체크섬을 수행\\n하는 것은 알고리즘을 사용하여 파일의 모든 바이트를 순차적으로 반복하는 작업으로 구성됩니다. \\nAmazon S3는 데이터 무결성을 검사하기 위한 여러 체크섬 옵션을 제공합니다. 내구성 모범 사례로 이\\n러한 무결성 검사를 수행하고 모든 바이트가 변경 없이 전송되는지 확인하는 것이 좋습니다. Amazon'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 348, 'total_pages': 3031}, page_content='러한 무결성 검사를 수행하고 모든 바이트가 변경 없이 전송되는지 확인하는 것이 좋습니다. Amazon \\nS3는 SHA-1, SHA-256, CRC32, CRC32C 등의 알고리즘도 지원합니다. Amazon S3는 이러한 알고리\\n멀티파트 업로드 사용 API 버전 2006-03-01 327'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 349, 'total_pages': 3031}, page_content='Amazon Simple Storage Service 사용 설명서\\n즘을 하나 이상 사용하여 추가 체크섬 값을 계산하고 객체 메타데이터의 일부로 저장합니다. 체크섬에 \\n대한 자세한 내용은 Amazon S3에서 객체 무결성 확인\\xa0섹션을 참조하십시오.\\n목표\\n이 튜토리얼에서는 AWS 명령줄 인터페이스(AWS CLI)를 통해 멀티파트 업로드 및 추가 SHA-256 체\\n크섬을 사용하여 Amazon S3에 객체를 업로드하는 방법을 알아봅니다. 또한 업로드한 객체의 MD5 해\\n시 및 SHA-256 체크섬을 계산하여 객체의 데이터 무결성을 검사하는 방법도 배우게 됩니다.\\n주제\\n• 사전 조건\\n• 1단계: 대용량 파일 생성\\n• 2단계: 파일을 여러 파일로 분할\\n• 3단계: 추가 체크섬을 사용하여 멀티파트 업로드 생성\\n• 4단계: 멀티파트 업로드의 부분 업로드\\n• 5단계: 멀티파트 업로드의 모든 부분 나열\\n• 6단계: 멀티파트 업로드 완료\\n• 7단계: 객체가 버킷에 업로드되었는지 확인\\n• 8단계: MD5 체크섬으로 객체 무결성 확인\\n• 9단계: 추가 체크섬으로 객체 무결성 확인\\n• 10단계: 리소스 정리\\n사전 조건'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 349, 'total_pages': 3031}, page_content='• 7단계: 객체가 버킷에 업로드되었는지 확인\\n• 8단계: MD5 체크섬으로 객체 무결성 확인\\n• 9단계: 추가 체크섬으로 객체 무결성 확인\\n• 10단계: 리소스 정리\\n사전 조건\\n• 이 튜토리얼을 시작하기 전에 업로드할 수 있는 Amazon S3 버킷에 대한 액세스 권한이 있는지 확인\\n합니다. 자세한 내용은 범용 버킷 생성 섹션을 참조하세요.\\n• AWS CLI를 설치하고 구성해야 합니다. AWS CLI를 설치하지 않은 경우 AWS Command Line \\nInterface 사용 설명서에서 최신 버전의 AWS CLI 설치 또는 업데이트를 참조하세요.\\n• 또는 AWS CloudShell을 사용하여 콘솔에서 AWS CLI 명령을 실행할 수 있습니다. AWS CloudShell\\n은 브라우저 기반의 사전 인증된 쉘로, AWS Management Console에서 직접 시작할 수 있습니다. 자\\n세한 내용은 AWS CloudShell 사용 설명서에서 CloudShell이란 무엇인가요? 및 AWS CloudShell 시\\n작하기를 참조하세요.\\n멀티파트 업로드 사용 API 버전 2006-03-01 328'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 350, 'total_pages': 3031}, page_content='Amazon Simple Storage Service 사용 설명서\\n1단계: 대용량 파일 생성\\n업로드할 파일이 이미 있으면 이 튜토리얼에 해당 파일을 사용할 수 있습니다. 그렇지 않은 경우 다음 \\n단계를 사용하여 15MB 파일을 생성합니다. 멀티파트 업로드와 관련된 제한은 Amazon S3 멀티파트 \\n업로드 제한 섹션을 참조하세요.\\n대용량 파일을 생성하려면\\n사용 중인 운영 체제에 따라 다음 명령 중 하나로 파일을 생성합니다.\\nLinux 또는 macOS\\n15MB 파일을 생성하려면 로컬 터미널을 열고 다음 명령을 실행합니다.\\ndd if=/dev/urandom of=census-data.bin bs=1M count=15\\n이 명령은 크기가 15MB인 무작위 바이트로 채워진 census-data.bin 파일을 생성합니다.\\nWindows\\n15MB 파일을 생성하려면 로컬 터미널을 열고 다음 명령을 실행합니다.\\nfsutil file createnew census-data.bin 15728640\\n이 명령은 크기가 15MB((15728640바이트)인 임의 데이터로 이루어진 census-data.bin 파일을 생\\n성합니다.\\n2단계: 파일을 여러 파일로 분할'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 350, 'total_pages': 3031}, page_content='이 명령은 크기가 15MB((15728640바이트)인 임의 데이터로 이루어진 census-data.bin 파일을 생\\n성합니다.\\n2단계: 파일을 여러 파일로 분할\\n멀티파트 업로드를 수행하려면 대용량 파일을 더 작은 부분으로 분할해야 합니다. 그런 다음 멀티파트 \\n업로드 프로세스를 사용하여 작은 부분을 업로드할 수 있습니다. 이 단계에서는 1단계에서 생성한 대\\n용량 파일을 작은 부분으로 분할하는 방법을 보여줍니다. 다음 예는 census-data.bin이라는 15MB \\n파일을 사용합니다.\\n대용량 파일을 여러 부분으로 분할하려면\\nLinux 또는 macOS\\n대용량 파일을 5MB 부분으로 나누려면 split 명령을 사용합니다. 터미널을 열고 다음 명령을 실행합\\n니다.\\nsplit -b 5M -d census-data.bin census-part\\n멀티파트 업로드 사용 API 버전 2006-03-01 329'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 351, 'total_pages': 3031}, page_content='Amazon Simple Storage Service 사용 설명서\\n이 명령은 census-data.bin을 census-part**라는 5MB 부분으로 나뉩니다. 여기서 **는 00으\\n로 시작하는 숫자 접미사입니다.\\nWindows\\n대용량 파일을 분할하려면 PowerShell을 사용합니다. PowerShell을 열고 다음 스크립트를 실행합니\\n다.\\n$inputFile = \"census-data.bin\"\\n$outputFilePrefix = \"census-part\"\\n$chunkSize = 5MB\\n$fs = [System.IO.File]::OpenRead($inputFile)\\n$buffer = New-Object byte[] $chunkSize\\n$fileNumber = 0\\nwhile ($fs.Position -lt $fs.Length) {\\n$bytesRead = $fs.Read($buffer, 0, $chunkSize)\\n$outputFile = \"{0}{1:D2}\" -f $outputFilePrefix, $fileNumber\\n$fileStream = [System.IO.File]::Create($outputFile)'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 351, 'total_pages': 3031}, page_content='$fileStream = [System.IO.File]::Create($outputFile)\\n$fileStream.Write($buffer, 0, $bytesRead)\\n$fileStream.Close()\\n$fileNumber++\\n}\\n$fs.Close()\\n이 PowerShell 스크립트는 5MB 청크로 대용량 파일을 읽고 각 청크를 숫자 접미사가 있는 새 파일에 \\n씁니다.\\n적절한 명령을 실행한 후에는 명령을 실행한 디렉터리에 해당 부분이 표시됩니다. 각 부분에는 부분 번\\n호에 해당하는 접미사가 붙습니다. 예를 들면 다음과 같습니다.\\ncensus-part00 census-part01 census-part02\\n3단계: 추가 체크섬을 사용하여 멀티파트 업로드 생성\\n멀티파트 업로드 프로세스를 시작하려면 멀티파트 업로드 요청을 생성해야 합니다. 이 단계에는 멀티\\n파트 업로드를 시작하고 데이터 무결성을 위한 추가 체크섬을 지정하는 작업이 포함됩니다. 다음 예에\\n서는 SHA-256 체크섬을 사용합니다. 업로드할 객체에 메타데이터를 제공하려는 경우 멀티파트 업로\\n드 시작 요청에서 메타데이터를 제공해야 합니다.\\n멀티파트 업로드 사용 API 버전 2006-03-01 330'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 352, 'total_pages': 3031}, page_content='Amazon Simple Storage Service 사용 설명서\\nNote\\n이 단계와 후속 단계에서 이 튜토리얼은 SHA-256 추가 알고리즘을 사용합니다. 필요에 따라 \\n이러한 단계에 CRC32, CRC32C 또는 SHA-1을 비롯한 다른 추가 체크섬을 사용할 수도 있습\\n니다. 다른 알고리즘을 사용하는 경우 튜토리얼 단계 전체에서 사용해야 합니다.\\n멀티파트 업로드를 시작하려면\\n터미널에서 다음 create-multipart-upload 명령을 사용하여 버킷의 멀티파트 업로드를 시작합\\n니다. amzn-s3-demo-bucket1을 실제 버킷 이름으로 바꿉니다. 또한 census_data_file을 선택\\n한 파일 이름으로 바꿉니다. 업로드가 완료되면 이 파일 이름이 객체 키가 됩니다.\\naws s3api create-multipart-upload --bucket amzn-s3-demo-bucket1 --key \\n \\'census_data_file\\' --checksum-algorithm sha256\\n요청이 성공하면 다음과 같은 JSON 결과가 표시됩니다.\\n{ \\n    \"ServerSideEncryption\": \"AES256\",'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 352, 'total_pages': 3031}, page_content='요청이 성공하면 다음과 같은 JSON 결과가 표시됩니다.\\n{ \\n    \"ServerSideEncryption\": \"AES256\", \\n    \"ChecksumAlgorithm\": \"SHA256\", \\n    \"Bucket\": \"amzn-s3-demo-bucket1\", \\n    \"Key\": \"census_data_file\", \\n    \"UploadId\": \\n \"cNV6KCSNANFZapz1LUGPC5XwUVi1n6yUoIeSP138sNOKPeMhpKQRrbT9k0ePmgoOTCj9K83T4e2Gb5hQvNoNpCKqyb8m3.oyYgQNZD6FNJLBZluOIUyRE.qM5yhDTdhz\"\\n} \\n         \\nNote\\n멀티파트 업로드 시작 요청을 전송하면 Amazon S3는 멀티파트 업로드에 대한 고유 식별자인 \\n업로드 ID와 함께 응답을 반환합니다. 부분 업로드, 부분 목록 확인, 업로드 완료 또는 업로드 \\n중단 요청 시 항상 이 업로드 ID를 포함해야 합니다. 이후 단계에서 UploadId, Key, Bucket\\n값을 사용해야 하므로 이러한 값을 저장해 둡니다.\\n추가 체크섬과 함께 멀티파트 업로드를 사용하는 경우 부분 번호는 연속된 번호여야 합니다.'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 352, 'total_pages': 3031}, page_content='값을 사용해야 하므로 이러한 값을 저장해 둡니다.\\n추가 체크섬과 함께 멀티파트 업로드를 사용하는 경우 부분 번호는 연속된 번호여야 합니다. \\n연속되지 않은 부분 번호를 사용하는 경우 complete-multipart-upload 요청에서 HTTP\\n500 Internal Server Error가 반환될 수 있습니다.\\n멀티파트 업로드 사용 API 버전 2006-03-01 331'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 353, 'total_pages': 3031}, page_content='Amazon Simple Storage Service 사용 설명서\\n4단계: 멀티파트 업로드의 부분 업로드\\n이 단계에서는 S3 버킷에 멀티파트 업로드의 부분들을 업로드합니다. upload-part 명령을 사용하여 \\n각 부분을 개별적으로 업로드합니다. 이 프로세스를 수행하려면 업로드 ID, 부분 번호, 각 부분에 업로\\n드할 파일을 지정해야 합니다.\\n부분을 업로드하려면\\n1. 부분을 업로드할 때 업로드 ID와 함께 --part-number 인수를 사용하여 부분 번호를 지정해야 \\n합니다. 1부터 10,000까지 부분 번호를 지정할 수 있습니다. 부분 번호를 사용하여 업로드하는 객\\n체에서 각 부분과 그 위치를 고유하게 식별합니다. 부분 번호는 연속 시퀀스로 선택해야 합니다\\n(예: 1, 2 또는 3일 수 있음). 이전에 업로드한 부분과 동일한 부분 번호로 새 부분을 업로드할 경우 \\n이전에 업로드한 부분을 덮어쓰게 됩니다.\\n2. upload-part 명령을 사용하여 멀티파트 업로드의 각 부분을 업로드합니다. --upload-id는 3\\n단계에서 create-multipart-upload 명령으로 생성된 출력과 동일합니다. 데이터의 첫 부분'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 353, 'total_pages': 3031}, page_content='단계에서 create-multipart-upload 명령으로 생성된 출력과 동일합니다. 데이터의 첫 부분\\n을 업로드하려면 다음 명령을 사용합니다.\\naws s3api upload-part --bucket amzn-s3-demo-bucket1 --key \\n \\'census_data_file\\' --part-number 1 --body census-part00 --upload-id \\n \"cNV6KCSNANFZapz1LUGPC5XwUVi1n6yUoIeSP138sNOKPeMhpKQRrbT9k0ePmgoOTCj9K83T4e2Gb5hQvNoNpCKqyb8m3.oyYgQNZD6FNJLBZluOIUyRE.qM5yhDTdhz\" \\n --checksum-algorithm SHA256\\n각 upload-part 명령을 완료하면 다음 예와 같은 출력이 표시됩니다.\\n{ \\n    \"ServerSideEncryption\": \"AES256\", \\n    \"ETag\": \"\\\\\"e611693805e812ef37f96c9937605e69\\\\\"\", \\n    \"ChecksumSHA256\": \"QLl8R4i4+SaJlrl8ZIcutc5TbZtwt2NwB8lTXkd3GH0=\"\\n}'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 353, 'total_pages': 3031}, page_content='\"ChecksumSHA256\": \"QLl8R4i4+SaJlrl8ZIcutc5TbZtwt2NwB8lTXkd3GH0=\"\\n} \\n             \\n3. 후속 부분의 경우 부분 번호를 적절히 증가시킵니다.\\naws s3api upload-part --bucket amzn-s3-demo-bucket1 --key \\'census_data_file\\' --\\npart-number <part-number> --body <file-path> --upload-id \"<your-upload-id>\" --\\nchecksum-algorithm SHA256\\n예를 들어, 다음 명령을 사용하여 두 번째 부분을 업로드합니다.\\n멀티파트 업로드 사용 API 버전 2006-03-01 332'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 354, 'total_pages': 3031}, page_content='Amazon Simple Storage Service 사용 설명서\\naws s3api upload-part --bucket amzn-s3-demo-bucket1 --key \\n \\'census_data_file\\' --part-number 2 --body census-part01 --upload-id \\n \"cNV6KCSNANFZapz1LUGPC5XwUVi1n6yUoIeSP138sNOKPeMhpKQRrbT9k0ePmgoOTCj9K83T4e2Gb5hQvNoNpCKqyb8m3.oyYgQNZD6FNJLBZluOIUyRE.qM5yhDTdhz\" \\n --checksum-algorithm SHA256\\nAmazon S3는 업로드된 각 부분에 대한 엔터티 태그(ETag)와 추가 체크섬을 응답의 헤더로 반환\\n합니다.\\n4. 객체의 모든 부분을 업로드할 때까지 upload-part 명령을 계속 사용합니다.\\n5단계: 멀티파트 업로드의 모든 부분 나열\\n멀티파트 업로드를 완료하려면 해당 멀티파트 업로드를 위해 업로드된 모든 부분의 목록이 필요합니\\n다. list-parts 명령의 출력에는 버킷 이름, 키, 업로드 ID, 파트 번호, ETag, 추가 체크섬 등과 같은'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 354, 'total_pages': 3031}, page_content=\"다. list-parts 명령의 출력에는 버킷 이름, 키, 업로드 ID, 파트 번호, ETag, 추가 체크섬 등과 같은 \\n정보가 제공됩니다. 멀티파트 업로드 프로세스를 완료할 때 다음 단계에서 사용할 수 있도록 이 출력을 \\n파일에 저장하는 것이 좋습니다. 다음 방법을 사용하여 parts.json이라는 JSON 출력 파일을 생성할 \\n수 있습니다.\\n모든 부분이 나열된 파일을 생성하려면\\n1. 업로드된 모든 부분의 세부 정보가 포함된 JSON 파일을 생성하려면 다음 list-parts 명령을 사\\n용합니다. amzn-s3-demo-bucket1을 실제 버킷 이름 및 <your-upload-id>(3단계에서 받은 \\n업로드 ID)로 바꿉니다 list-parts 명령에 대한 자세한 내용은 AWS Command Line Interface \\n사용 설명서의 list-parts를 참조하세요.\\naws s3api list-parts --bucket amzn-s3-demo-bucket1 --key 'census_data_file' --\"),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 354, 'total_pages': 3031}, page_content='aws s3api list-parts --bucket amzn-s3-demo-bucket1 --key \\'census_data_file\\' --\\nupload-id <your-upload-id> --query \\'{Parts: Parts[*].{PartNumber: PartNumber, ETag: \\n ETag, ChecksumSHA256: ChecksumSHA256}}\\' --output json > parts.json\\nparts.json이라는 새 파일이 생성됩니다. 이 파일에는 업로드한 모든 부분에 대한 JSON 형식의 \\n정보가 들어 있습니다. parts.json 파일에는 멀티파트 업로드 프로세스를 완료하는 데 필요한 \\n부분 번호, 해당 ETag 값 등 멀티파트 업로드의 각 부분에 대한 필수 정보가 포함되어 있습니다.\\n2. 텍스트 편집기를 사용하거나 터미널을 통해 parts.json을 열 수 있습니다. 예시 출력은 다음과 \\n같습니다.\\n{ \\n    \"Parts\": [ \\n멀티파트 업로드 사용 API 버전 2006-03-01 333'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 355, 'total_pages': 3031}, page_content='Amazon Simple Storage Service 사용 설명서\\n        { \\n            \"PartNumber\": 1, \\n            \"ETag\": \"\\\\\"3c3097f89e2a2fece47ac54b243c9d97\\\\\"\", \\n            \"ChecksumSHA256\": \"fTPVHfyNHdv5VkR4S3EewdyioXECv7JBxN+d4FXYYTw=\" \\n        }, \\n        { \\n            \"PartNumber\": 2, \\n            \"ETag\": \"\\\\\"03c71cc160261b20ab74f6d2c476b450\\\\\"\", \\n            \"ChecksumSHA256\": \"VDWTa8enjOvULBAO3W2a6C+5/7ZnNjrnLApa1QVc3FE=\" \\n        }, \\n        { \\n            \"PartNumber\": 3, \\n            \"ETag\": \"\\\\\"81ae0937404429a97967dffa7eb4affb\\\\\"\", \\n            \"ChecksumSHA256\": \"cVVkXehUlzcwrBrXgPIM+EKQXPUvWist8mlUTCs4bg8=\"'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 355, 'total_pages': 3031}, page_content='\"ChecksumSHA256\": \"cVVkXehUlzcwrBrXgPIM+EKQXPUvWist8mlUTCs4bg8=\" \\n        } \\n    ]\\n} \\n             \\n6단계: 멀티파트 업로드 완료\\n멀티파트 업로드의 모든 부분을 업로드하고 나열한 후, 마지막 단계는 멀티파트 업로드를 완료하는 것\\n입니다. 이 단계는 업로드된 모든 부분을 S3 버킷의 단일 객체로 병합합니다.\\nNote\\n요청에 --checksum-sha256을 포함하여 complete-multipart-upload를 직접적으로 \\n호출하기 전에 객체 체크섬을 계산할 수 있습니다. 체크섬이 일치하지 않으면 Amazon S3가 \\n요청에 실패합니다. 자세한 내용은 AWS Command Line Interface 사용 설명서의 complete-\\nmultipart-upload를\\xa0참조하세요.\\n멀티파트 업로드를 완료하려면\\n멀티파트 업로드를 완료하려면 complete-multipart-upload 명령을 사용합니다. 이 명령에\\n는 5단계에서 생성한 parts.json 파일, 버킷 이름, 업로드 ID가 필요합니다. <amzn-s3-demo-'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 355, 'total_pages': 3031}, page_content='는 5단계에서 생성한 parts.json 파일, 버킷 이름, 업로드 ID가 필요합니다. <amzn-s3-demo-\\nbucket1>을 해당 버킷 이름으로 바꾸고 <your-upload-id>를 parts.json의 업로드 ID로 바꿉니\\n다.\\n멀티파트 업로드 사용 API 버전 2006-03-01 334'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 356, 'total_pages': 3031}, page_content='Amazon Simple Storage Service 사용 설명서\\naws s3api complete-multipart-upload --multipart-upload file://parts.json --bucket amzn-\\ns3-demo-bucket1 --key \\'census_data_file\\' --upload-id <your-upload-id>\\n예시 출력은 다음과 같습니다.\\n{ \\n    \"ServerSideEncryption\": \"AES256\", \\n    \"Location\": \"https://amzn-s3-demo-bucket1.s3.us-east-2.amazonaws.com/\\ncensus_data_file\", \\n    \"Bucket\": \"amzn-s3-demo-bucket1\", \\n    \"Key\": \"census_data_file\", \\n    \"ETag\": \"\\\\\"f453c6dccca969c457efdf9b1361e291-3\\\\\"\", \\n    \"ChecksumSHA256\": \"aI8EoktCdotjU8Bq46DrPCxQCGuGcPIhJ51noWs6hvk=-3\"\\n} \\n             \\nNote'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 356, 'total_pages': 3031}, page_content='\"ChecksumSHA256\": \"aI8EoktCdotjU8Bq46DrPCxQCGuGcPIhJ51noWs6hvk=-3\"\\n} \\n             \\nNote\\n개별 부분 파일은 아직 삭제하지 마세요. 함께 병합된 객체의 무결성을 확인하기 위해 해당 부\\n분에 대해 체크섬을 수행하려면 개별 부분이 필요합니다.\\n7단계: 객체가 버킷에 업로드되었는지 확인\\n멀티파트 업로드를 완료한 후 객체가 S3 버킷에 성공적으로 업로드되었는지 확인할 수 있습니다. 버\\n킷의 객체를 나열하고 새로 업로드한 파일이 있는지 확인하려면 list-objects-v2 명령을 사용합니\\n다.\\n업로드된 객체를 나열하려면\\n버킷의 객체를 나열하려면 list-objects-v2 명령을 사용합니다. amzn-s3-demo-bucket1을 실\\n제 버킷 이름으로 바꿉니다.\\naws s3api list-objects-v2 --bucket amzn-s3-demo-bucket1\\n이 명령은 버킷의 객체 목록을 반환합니다. 객체 목록에서 업로드한 파일(예:census_data_file)을 \\n찾아보세요.'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 356, 'total_pages': 3031}, page_content='이 명령은 버킷의 객체 목록을 반환합니다. 객체 목록에서 업로드한 파일(예:census_data_file)을 \\n찾아보세요.\\n자세한 내용은 AWS Command Line Interface 사용 설명서에서 list-objects-v2 명령에 대한 예제\\n섹션을 참조하세요.\\n멀티파트 업로드 사용 API 버전 2006-03-01 335'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 357, 'total_pages': 3031}, page_content='Amazon Simple Storage Service 사용 설명서\\n8단계: MD5 체크섬으로 객체 무결성 확인\\n객체를 업로드할 때 Amazon S3에서 사용할 체크섬 알고리즘을 지정할 수 있습니다. 기본적으로 \\nAmazon S3는 MD5 바이트 다이제스트를 객체의 ETag로 저장합니다. 멀티파트 업로드의 경우 ETag\\n는 전체 객체의 체크섬이 아니라 각 개별 부분에 대한 체크섬의 복합체입니다.\\nMD5 체크섬을 사용하여 객체 무결성을 확인하려면\\n1. 업로드된 객체의 ETag를 검색하려면 head-object 요청을 수행합니다.\\naws s3api head-object --bucket amzn-s3-demo-bucket1 --key census_data_file\\n예시 출력은 다음과 같습니다.\\n{ \\n    \"AcceptRanges\": \"bytes\", \\n    \"LastModified\": \"2024-07-26T19:04:13+00:00\", \\n    \"ContentLength\": 16106127360, \\n    \"ETag\": \"\\\\\"f453c6dccca969c457efdf9b1361e291-3\\\\\"\",'),\n",
       " Document(metadata={'source': './s3-userguide.pdf', 'page': 357, 'total_pages': 3031}, page_content='\"ContentLength\": 16106127360, \\n    \"ETag\": \"\\\\\"f453c6dccca969c457efdf9b1361e291-3\\\\\"\", \\n    \"ContentType\": \"binary/octet-stream\", \\n    \"ServerSideEncryption\": \"AES256\", \\n    \"Metadata\": {}\\n} \\n             \\n이 ETag의 끝에는 ‘-3’이 추가되어 있습니다. 이는 멀티파트 업로드를 사용하여 객체가 세 부분으\\n로 업로드되었음을 나타냅니다.\\n2. 그런 다음 md5sum 명령을 사용하여 각 부분의 MD5 체크섬을 계산합니다. 부분 파일에 올바른 경\\n로를 입력했는지 확인합니다.\\nmd5sum census-part*\\n예시 출력은 다음과 같습니다.\\ne611693805e812ef37f96c9937605e69 census-part00\\n63d2d5da159178785bfd6b6a5c635854 census-part01\\n95b87c7db852451bb38b3b44a4e6d310 census-part02 \\n멀티파트 업로드 사용 API 버전 2006-03-01 336')]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs[1000:1050]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5580d4db-458a-44e1-bd85-d28cf617395e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 0 indexed successfully\n",
      "Document 500 indexed successfully\n",
      "Document 1000 indexed successfully\n"
     ]
    }
   ],
   "source": [
    "save_chunk(opensearch_client, \"s3_explain\" ,  split_docs[1000:2500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "af5be344-c02e-49d9-aa2b-3391c2afd5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BM25 search ( nori_analyzer를 이용하여 형태소 분석을 진행한후 텍스트 유사도 측정 )\n",
    "def simple_text_search(client, search_text, index_name=\"s3_explain\", k=10):\n",
    "    try:\n",
    "        # Simple text search query\n",
    "        text_query = {\n",
    "            \"size\": k,  # Number of results to return\n",
    "            \"_source\": {\n",
    "                \"excludes\": [\"doc_vector\"]  # Exclude vector field from results\n",
    "            },\n",
    "            \"query\": {\n",
    "                \"match\": {\n",
    "                    \"document\": {\n",
    "                        \"query\": search_text,\n",
    "                        \"analyzer\": \"nori_analyzer\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Execute search\n",
    "        response = client.search(\n",
    "            index=index_name,\n",
    "            body=text_query\n",
    "        )\n",
    "\n",
    "\n",
    "        documents = []\n",
    "        if response.get(\"hits\", {}).get(\"hits\", []):\n",
    "            search_results = normalize_search_results(response) \n",
    "            for res in search_results[\"hits\"][\"hits\"]:\n",
    "                source = res['_source']\n",
    "                page_content = {k: source[k] for k in source if k != \"table_summary_v\"}\n",
    "                metadata = {\"id\": res['_id']}\n",
    "                score = res['_score']  # Get the score from the search result\n",
    "                documents.append((Document(page_content=json.dumps(page_content, ensure_ascii=False), metadata=metadata), score))\n",
    "        return documents  \n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "#BM25 search의 score는 0.1 to 15.0 임으로 normazlie(0~1)를 해야함 (추후 vector search와 score 비교를 하기위해)\n",
    "def normalize_search_results(search_results):\n",
    "        hits = (search_results[\"hits\"][\"hits\"])\n",
    "        max_score = float(search_results[\"hits\"][\"max_score\"])\n",
    "        for hit in hits:\n",
    "            hit[\"_score\"] = float(hit[\"_score\"]) / max_score\n",
    "        search_results[\"hits\"][\"max_score\"] = hits[0][\"_score\"]\n",
    "        search_results[\"hits\"][\"hits\"] = hits\n",
    "        return search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "429d1369-34e0-4602-84f8-a3d69c4945b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector 유사도 검색 \n",
    "def vector_search(client, embedding_vector, index_name=\"s3_explain\", k=3):\n",
    "    try:\n",
    "        # KNN vector search query\n",
    "        vector_query = {\n",
    "            \"size\": k,\n",
    "            \"_source\": {\n",
    "                \"excludes\": [\"doc_vector\"]  # Exclude vector field from results\n",
    "            },\n",
    "            \"query\": {\n",
    "                \"knn\": {\n",
    "                    \"doc_vector\": {\n",
    "                        \"vector\": embedding_vector,\n",
    "                        \"k\": k\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "        # Execute the search\n",
    "        response = client.search(\n",
    "            index=index_name,\n",
    "            body=vector_query\n",
    "        )\n",
    "\n",
    "        documents = []\n",
    "        for res in response[\"hits\"][\"hits\"]:\n",
    "            source = res['_source']\n",
    "            page_content = {k: source[k] for k in source if k != \"vector\"}\n",
    "            metadata = {\"id\": res['_id']}  # Add metadata with a unique identifier\n",
    "            score = res['_score']  # Get the match score from the search result\n",
    "            documents.append((Document(page_content=json.dumps(page_content, ensure_ascii=False), metadata=metadata), score))\n",
    "        return documents\n",
    "            \n",
    "\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Search error: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b60505c6-e3d4-4144-9f2f-e21fb6e0de73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#노리를 통해 검색한 10개 + vector 유사도 기반으로 검색한 10개 총 20개를 정렬\n",
    "def get_ensemble_results(doc_lists: List[List[Tuple[Document, float]]], weights: List[float], k: int = 5) -> List[Document]:\n",
    "        hybrid_score_dic: Dict[str, float] = {}\n",
    "        doc_map: Dict[str, Document] = {}\n",
    "        \n",
    "        # Weight-based adjustment\n",
    "        for doc_list, weight in zip(doc_lists, weights):\n",
    "            for doc, score in doc_list:\n",
    "                doc_id = doc.metadata.get(\"id\", doc.page_content)\n",
    "                if doc_id not in hybrid_score_dic:\n",
    "                    hybrid_score_dic[doc_id] = 0.0\n",
    "                hybrid_score_dic[doc_id] += score * weight\n",
    "                doc_map[doc_id] = doc\n",
    "    \n",
    "        sorted_docs = sorted(hybrid_score_dic.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [doc_map[doc_id] for doc_id, _ in sorted_docs[:k]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6fa43228-9cce-48f4-b860-c807c5e07b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#하이브리드 서치 코드 -> 노리를 통해 검색한 10개 + vector 유사도 기반으로 검색한 10개 총 20개에서 rerank를 통해 최종 10개만 선택한다\n",
    "def retrieval_augmented(query , k =10): \n",
    "    \n",
    "    #embedding \n",
    "    raw_embedding = get_embedding(query)\n",
    "    #vector search 가져오기 \n",
    "    vector=vector_search(opensearch_client, raw_embedding ,k =k)\n",
    "    #lexical search 가져오기 \n",
    "    lexical=simple_text_search(opensearch_client, query ,k =k)\n",
    "\n",
    "    #rerank\n",
    "    rerank_doc = get_ensemble_results(\n",
    "            doc_lists=[vector, lexical],\n",
    "            weights= [0.7, 0.3],\n",
    "            k=k,\n",
    "    )\n",
    "    return rerank_doc\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "01d4d8b4-ac1a-4500-b6ca-6afc6c0f9413",
   "metadata": {},
   "outputs": [],
   "source": [
    "#결과 검색\n",
    "result =retrieval_augmented(\"s3 에 대해 설명해줘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ffc017b8-af7d-4708-ba09-47f48967e25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'id': 'sagemaker_doc_1386'}, page_content='{\"document\": \"경우 개별 디렉터리는 디렉터리 버킷의 최대 요청 속도를 지원하도록 설계되었습니다. 시스템이 균등\\\\n한 로드 분배를 위해 객체를 자동으로 분산하므로 최적의 성능을 달성하기 위해 키 접두사를 무작위화\\\\n고성능 워크로드 API 버전 2006-03-01 818\", \"source\": \"./s3-userguide.pdf\", \"page\": 839, \"total_pages\": 3031}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_331'}, page_content='{\"document\": \"5. 삭제를 선택합니다.\\\\n6. 객체 삭제 페이지에서 삭제하기 위해 선택한 폴더와 객체의 이름이 지정된 객체에 나열되어 있는\\\\n지 확인합니다.\\\\n폴더 사용 API 버전 2006-03-01 432\", \"source\": \"./s3-userguide.pdf\", \"page\": 453, \"total_pages\": 3031}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_1117'}, page_content='{\"document\": \"12. S3 인벤토리에 대한 권한을 부여하려면 정책 필드에서 다음 버킷 정책을 붙여 넣습니다.\\\\n세 가지 예제 값을 다음 값으로 바꿉니다.\\\\n자습서: 비디오 일괄 트랜스코딩 API 버전 2006-03-01 706\", \"source\": \"./s3-userguide.pdf\", \"page\": 727, \"total_pages\": 3031}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_416'}, page_content='{\"document\": \"을 입력합니다.\\\\n표준 액세스 포인트와 마찬가지로 객체 Lambda 액세스 포인트에 대한 명명 규칙이 있습니다. 자\\\\n세한 내용은 액세스 포인트 이름 지정 규칙 섹션을 참조하세요.\\\\n6. Supporting Access Point(지원 액세스 지점)에 사용하려는 표준 액세스 지점을 입력하거나 찾아봅\\\\n니다. 액세스 포인트는 변환하려는 객체와 동일한 AWS 리전에 있어야 합니다. 표준 액세스 지점 \\\\n생성에 대한 자세한 내용은 the section called “액세스 포인트 생성” 섹션을 참조하세요.\\\\n7. 변환 구성에서 객체 Lambda 액세스 포인트의 데이터를 변환하는 함수를 추가할 수 있습니다. 다\\\\n음 중 하나를 수행합니다.\\\\n• 계정에 이미 AWS Lambda 함수가 있는 경우 Invoke Lambda function(Lambda 함수 호출)에서 \\\\n해당 함수를 선택할 수 있습니다. 여기에서 AWS 계정에 있는 Lambda 함수의 Amazon 리소스 \\\\n이름(ARN)을 입력하거나 드롭다운 메뉴에서 Lambda 함수를 선택할 수 있습니다.\\\\n• AWS에서 빌드한 함수를 사용하려면 AWS built function( 빌드 함수)에서 함수 이름을 선택하고\", \"source\": \"./s3-userguide.pdf\", \"page\": 485, \"total_pages\": 3031}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_1271'}, page_content='{\"document\": \"있습니다. 가용 영역의 디렉터리 버킷은 S3 Express One Zone 스토리지 클래스를 지원합니다. S3 \\\\nExpress One Zone 스토리지 클래스는 애플리케이션이 성능에 민감하고 10밀리초 미만의 PUT 및 GET\\\\n지연 시간이 필요한 경우에 권장됩니다. 가용 영역에서 디렉터리 버킷을 생성하는 방법에 대한 자세한 \\\\n내용은 고성능 워크로드 섹션을 참조하세요.\\\\n데이터 레지던시 사용 사례의 경우 단일 AWS 전용 로컬 영역(DLZ)에 디렉터리 버킷을 생성하여 데\\\\n이터를 저장할 수 있습니다. 로컬 영역의 디렉터리 버킷은 S3 One Zone-Infrequent Access(S3 One \\\\nZone-IA, Z-IA) 스토리지 클래스를 지원합니다. 로컬 영역에서 디렉터리 버킷을 생성하는 방법에 대한 \\\\n자세한 내용은 데이터 레지던시 워크로드 섹션을 참조하세요.\\\\n주제\\\\n디렉터리 버킷 사용 사례 API 버전 2006-03-01 779\", \"source\": \"./s3-userguide.pdf\", \"page\": 800, \"total_pages\": 3031}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_1286'}, page_content='{\"document\": \"또한 IAM을 사용하여 CreateSession API 작업에 대한 액세스를 제어할 수 있으며, 이를 통해 리\\\\n전 또는 객체 관리 API 작업을 사용할 수 있습니다. 영역 API 작업에 대한 동일 계정 또는 크로스 계\\\\n정 액세스 권한을 부여할 수 있습니다. S3 Express One Zone 권한 및 정책에 대한 자세한 내용은\\\\nIAM을 사용한 리전 엔드포인트 API 권한 부여 섹션을 참조하세요.\\\\n• IAM Access Analyzer for S3 - 액세스 정책을 평가 및 모니터링하여 정책이 S3 리소스에 대한 의도된 \\\\n액세스만 제공하는지 확인합니다.\\\\n로깅 및 모니터링\\\\nS3 Express One Zone은 리소스가 사용되는 방식을 모니터링하고 제어하는 데 사용할 수 있는 다음의 \\\\nS3 로깅 및 모니터링 도구를 제공합니다.\\\\n• Amazon CloudWatch 지표 - CloudWatch를 사용하여 지표를 수집 및 추적하는 방식으로 AWS 리소\\\\n스 및 애플리케이션을 모니터링합니다. S3 Express One Zone은 다른 Amazon S3 스토리지 클래스\", \"source\": \"./s3-userguide.pdf\", \"page\": 805, \"total_pages\": 3031}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_1284'}, page_content='{\"document\": \"원되지 않습니다. 디렉토리 버킷은 S3 객체 소유권에 대해 버킷 소유자 적용 설정을 자동으로 사용\\\\n고성능 워크로드 API 버전 2006-03-01 783\", \"source\": \"./s3-userguide.pdf\", \"page\": 804, \"total_pages\": 3031}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_187'}, page_content='{\"document\": \"번호나 바이트 범위를 지정하여 개별 파트의 체크섬을 가져올 수도 있습니다. 아직 진행 중인 멀티파\\\\n트 업로드의 개별 부분에 대한 체크섬 값을 검색하려면 ListParts를 사용하면 됩니다.\\\\n• Amazon S3가 멀티파트 객체에 대한 체크섬을 계산하는 방식 때문에 객체를 복사할 경우 객체의 체\\\\n크섬 값이 변경될 수 있습니다. SDK 또는 REST API를 사용하고 있으며 CopyObject를 직접 호출\\\\n하는 경우 Amazon S3는 CopyObject API 작업의 크기 한도에 도달할 때까지 객체를 복사합니다. \\\\nAmazon S3는 객체가 단일 요청으로 업로드되었는지 또는 멀티파트 업로드의 일부로 업로드되었는\\\\n지 여부에 관계없이 이 복사 작업을 단일 작업으로 수행합니다. 복사 명령을 사용하면 객체의 체크섬\\\\n은 전체 객체의 직접 체크섬입니다. 객체가 원래 멀티파트 업로드를 사용하여 업로드된 경우 데이터\\\\n가 변경되지 않아도 체크섬 값이 변경됩니다.\\\\n• CopyObject API 작업의 크기 한도보다 큰 객체는 멀티파트 업로드 복사 명령을 사용해야 합니다.\", \"source\": \"./s3-userguide.pdf\", \"page\": 407, \"total_pages\": 3031}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_1304'}, page_content='{\"document\": \"사용하여 사용자 또는 역할을 생성하고 해당 ID에 권한을 연결하면 됩니다. IAM 사용자를 생성하는 방\\\\n법에 대한 자세한 내용은 IAM 사용 설명서의 IAM 사용자 생성(콘솔)을 참조하십시오. IAM 역할을 생성\\\\n고성능 워크로드 API 버전 2006-03-01 789\", \"source\": \"./s3-userguide.pdf\", \"page\": 810, \"total_pages\": 3031}'),\n",
       " Document(metadata={'id': 'sagemaker_doc_1340'}, page_content='{\"document\": \"연 시간이 짧은 검색을 최적화하기 위해 Amazon S3 Express One Zone 스토리지 클래스의 객체는 컴\\\\n퓨팅 워크로드와 동일한 위치에 있는 로컬 단일 가용 영역의 S3 디렉터리 버킷에 중복 저장됩니다. 디\\\\n렉터리 버킷을 생성할 때 버킷이 위치할 가용 영역과 AWS 리전을 선택합니다.\\\\n고성능 워크로드 API 버전 2006-03-01 802\", \"source\": \"./s3-userguide.pdf\", \"page\": 823, \"total_pages\": 3031}')]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491dfd8c-e9f4-479a-8c1b-bc7ef5c72d96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c099b5-0fed-4b8f-a76c-f06622ecba08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cece2d87-0da9-465f-8b3a-ade4421d9a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fa4063-fe9c-463c-a44e-d641cc54399f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9145a-548a-4393-95a3-6b31559ca7fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1068fc66-e524-41de-9761-3d08093b8fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e87b4-883b-4f7c-b2c5-57654df88cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d178562-2351-4988-9ccd-c1b183663fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
